{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9c5213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "037bec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.util import Normalizer\n",
    "from model.database_util import get_hist_file, get_job_table_sample, collator\n",
    "from model.model import QueryFormer\n",
    "from model.database_util import Encoding\n",
    "from model.dataset import PlanTreeDataset\n",
    "from model.trainer import eval_workload, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "822fdcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/imdb/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbcd4773",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    # bs = 1024\n",
    "    # SQ: smaller batch size\n",
    "    bs = 128\n",
    "    lr = 0.001\n",
    "    # epochs = 200\n",
    "    epochs = 100\n",
    "    clip_size = 50\n",
    "    embed_size = 64\n",
    "    pred_hid = 128\n",
    "    ffn_dim = 128\n",
    "    head_size = 12\n",
    "    n_layers = 8\n",
    "    dropout = 0.1\n",
    "    sch_decay = 0.6\n",
    "    device = 'cuda:0'\n",
    "    newpath = './results/full/cost/'\n",
    "    to_predict = 'cost'\n",
    "args = Args()\n",
    "\n",
    "import os\n",
    "if not os.path.exists(args.newpath):\n",
    "    os.makedirs(args.newpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aace65f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shaikhq/QueryFormer/model/database_util.py:76: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  hist_file['freq'][i] = freq_np\n",
      "/home/shaikhq/QueryFormer/model/database_util.py:89: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  hist_file['bins'][rid] = \\\n",
      "/home/shaikhq/QueryFormer/model/database_util.py:89: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  hist_file['bins'][rid] = \\\n",
      "/home/shaikhq/QueryFormer/model/database_util.py:89: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  hist_file['bins'][rid] = \\\n",
      "/home/shaikhq/QueryFormer/model/database_util.py:89: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  hist_file['bins'][rid] = \\\n",
      "/home/shaikhq/QueryFormer/model/database_util.py:89: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  hist_file['bins'][rid] = \\\n",
      "/home/shaikhq/QueryFormer/model/database_util.py:89: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  hist_file['bins'][rid] = \\\n",
      "/home/shaikhq/QueryFormer/model/database_util.py:89: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  hist_file['bins'][rid] = \\\n",
      "/home/shaikhq/QueryFormer/model/database_util.py:89: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  hist_file['bins'][rid] = \\\n",
      "/home/shaikhq/QueryFormer/model/database_util.py:89: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  hist_file['bins'][rid] = \\\n"
     ]
    }
   ],
   "source": [
    "hist_file = get_hist_file(data_path + 'histogram_string.csv')\n",
    "cost_norm = Normalizer(-3.61192, 12.290855)\n",
    "card_norm = Normalizer(1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e5f421a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_ckpt = torch.load('checkpoints/encoding.pt')\n",
    "type(encoding_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adbb38f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['encoding'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_ckpt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb839f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model.database_util.Encoding"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = encoding_ckpt['encoding']\n",
    "type(encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3335b172",
   "metadata": {},
   "source": [
    "## Exploring Encoding object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "086ed48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'t.id': [1.0, 2528312.0], 't.kind_id': [1.0, 7.0], 't.production_year': [1880.0, 2019.0], 'mc.id': [1.0, 2609129.0], 'mc.company_id': [1.0, 234997.0], 'mc.movie_id': [2.0, 2525745.0], 'mc.company_type_id': [1.0, 2.0], 'ci.id': [1.0, 36244344.0], 'ci.movie_id': [1.0, 2525975.0], 'ci.person_id': [1.0, 4061926.0], 'ci.role_id': [1.0, 11.0], 'mi.id': [1.0, 14835720.0], 'mi.movie_id': [1.0, 2526430.0], 'mi.info_type_id': [1.0, 110.0], 'mi_idx.id': [1.0, 1380035.0], 'mi_idx.movie_id': [2.0, 2525793.0], 'mi_idx.info_type_id': [99.0, 113.0], 'mk.id': [1.0, 4523930.0], 'mk.movie_id': [2.0, 2525971.0], 'mk.keyword_id': [1.0, 134170.0]}\n"
     ]
    }
   ],
   "source": [
    "print(encoding.column_min_max_vals)\n",
    "# column_min_max_vals is a dictionary. It has the min and max value for each numeric column in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d42de580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'t.id': 0, 't.kind_id': 1, 't.production_year': 2, 'mc.id': 3, 'mc.company_id': 4, 'mc.movie_id': 5, 'mc.company_type_id': 6, 'ci.id': 7, 'ci.movie_id': 8, 'ci.person_id': 9, 'ci.role_id': 10, 'mi.id': 11, 'mi.movie_id': 12, 'mi.info_type_id': 13, 'mi_idx.id': 14, 'mi_idx.movie_id': 15, 'mi_idx.info_type_id': 16, 'mk.id': 17, 'mk.movie_id': 18, 'mk.keyword_id': 19, 'NA': 20}\n"
     ]
    }
   ],
   "source": [
    "print(encoding.col2idx)\n",
    "# the label encoding of each unique column in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8bd229d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'>': 0, '=': 1, '<': 2, 'NA': 3}\n"
     ]
    }
   ],
   "source": [
    "print(encoding.op2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5d23a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 't.id', 1: 't.kind_id', 2: 't.production_year', 3: 'mc.id', 4: 'mc.company_id', 5: 'mc.movie_id', 6: 'mc.company_type_id', 7: 'ci.id', 8: 'ci.movie_id', 9: 'ci.person_id', 10: 'ci.role_id', 11: 'mi.id', 12: 'mi.movie_id', 13: 'mi.info_type_id', 14: 'mi_idx.id', 15: 'mi_idx.movie_id', 16: 'mi_idx.info_type_id', 17: 'mk.id', 18: 'mk.movie_id', 19: 'mk.keyword_id', 20: 'NA'}\n"
     ]
    }
   ],
   "source": [
    "print(encoding.idx2col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cec6d6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gather': 0, 'Hash Join': 1, 'Seq Scan': 2, 'Hash': 3, 'Bitmap Heap Scan': 4, 'Bitmap Index Scan': 5, 'Nested Loop': 6, 'Index Scan': 7, 'Merge Join': 8, 'Gather Merge': 9, 'Materialize': 10, 'BitmapAnd': 11, 'Sort': 12}\n"
     ]
    }
   ],
   "source": [
    "print(encoding.type2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b660cb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('checkpoints/cost_model.pt', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71759b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.util import seed_everything\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9592f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QueryFormer(emb_size = args.embed_size ,ffn_dim = args.ffn_dim, head_size = args.head_size, \\\n",
    "                 dropout = args.dropout, n_layers = args.n_layers, \\\n",
    "                 use_sample = True, use_hist = True, \\\n",
    "                 pred_hid = args.pred_hid\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85e27584",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/QueryFormer/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/QueryFormer/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/QueryFormer/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/QueryFormer/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/QueryFormer/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:302\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    301\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 302\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    306\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver."
     ]
    }
   ],
   "source": [
    "_ = model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c8a7d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict = 'cost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4286ce8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_path = './data/imdb/'\n",
    "dfs = []  # list to hold DataFrames\n",
    "# SQ: added\n",
    "for i in range(2):\n",
    "#for i in range(18):\n",
    "    file = imdb_path + 'plan_and_cost/train_plan_part{}.csv'.format(i)\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "full_train_df = pd.concat(dfs)\n",
    "\n",
    "val_dfs = []  # list to hold DataFrames\n",
    "for i in range(18,20):\n",
    "    file = imdb_path + 'plan_and_cost/train_plan_part{}.csv'.format(i)\n",
    "    df = pd.read_csv(file)\n",
    "    val_dfs.append(df)\n",
    "\n",
    "val_df = pd.concat(val_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd8301be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded queries with len  100000\n",
      "Loaded bitmaps\n"
     ]
    }
   ],
   "source": [
    "table_sample = get_job_table_sample(imdb_path+'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f06ae89",
   "metadata": {},
   "source": [
    "## Exploring Histogram input (hist_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f377f08c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(hist_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4119781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table</th>\n",
       "      <th>column</th>\n",
       "      <th>freq</th>\n",
       "      <th>bins</th>\n",
       "      <th>table_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>title</td>\n",
       "      <td>production_year</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[1880, 1913, 1923, 1942, 1955, 1960, 1964, 196...</td>\n",
       "      <td>t.production_year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>title</td>\n",
       "      <td>kind_id</td>\n",
       "      <td>[0.0, 0.26216118191156074, 0.03593387047716835...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, ...</td>\n",
       "      <td>t.kind_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>movie_companies</td>\n",
       "      <td>company_id</td>\n",
       "      <td>[0.0, 0.0004959511376981121, 0.000558807386989...</td>\n",
       "      <td>[1, 6, 19, 27, 68, 133, 160, 189, 292, 402, 47...</td>\n",
       "      <td>mc.company_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>movie_companies</td>\n",
       "      <td>company_type_id</td>\n",
       "      <td>[0.0, 0.4883796425472418, 0.5116203574527581]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>mc.company_type_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cast_info</td>\n",
       "      <td>role_id</td>\n",
       "      <td>[0.0, 0.3495907485479872, 0.20560375449487386,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>ci.role_id</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             table           column  \\\n",
       "0            title  production_year   \n",
       "1            title          kind_id   \n",
       "2  movie_companies       company_id   \n",
       "3  movie_companies  company_type_id   \n",
       "4        cast_info          role_id   \n",
       "\n",
       "                                                freq  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.26216118191156074, 0.03593387047716835...   \n",
       "2  [0.0, 0.0004959511376981121, 0.000558807386989...   \n",
       "3      [0.0, 0.4883796425472418, 0.5116203574527581]   \n",
       "4  [0.0, 0.3495907485479872, 0.20560375449487386,...   \n",
       "\n",
       "                                                bins        table_column  \n",
       "0  [1880, 1913, 1923, 1942, 1955, 1960, 1964, 196...   t.production_year  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, ...           t.kind_id  \n",
       "2  [1, 6, 19, 27, 68, 133, 160, 189, 292, 402, 47...       mc.company_id  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  mc.company_type_id  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...          ci.role_id  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e108ba9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 5)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f590fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table</th>\n",
       "      <th>column</th>\n",
       "      <th>freq</th>\n",
       "      <th>bins</th>\n",
       "      <th>table_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>title</td>\n",
       "      <td>production_year</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[1880, 1913, 1923, 1942, 1955, 1960, 1964, 196...</td>\n",
       "      <td>t.production_year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>title</td>\n",
       "      <td>kind_id</td>\n",
       "      <td>[0.0, 0.26216118191156074, 0.03593387047716835...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, ...</td>\n",
       "      <td>t.kind_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>movie_companies</td>\n",
       "      <td>company_id</td>\n",
       "      <td>[0.0, 0.0004959511376981121, 0.000558807386989...</td>\n",
       "      <td>[1, 6, 19, 27, 68, 133, 160, 189, 292, 402, 47...</td>\n",
       "      <td>mc.company_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>movie_companies</td>\n",
       "      <td>company_type_id</td>\n",
       "      <td>[0.0, 0.4883796425472418, 0.5116203574527581]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>mc.company_type_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cast_info</td>\n",
       "      <td>role_id</td>\n",
       "      <td>[0.0, 0.3495907485479872, 0.20560375449487386,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>ci.role_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>movie_keyword</td>\n",
       "      <td>keyword_id</td>\n",
       "      <td>[0.0, 0.0031748950967179193, 1.989421142551088...</td>\n",
       "      <td>[1, 77, 132, 230, 331, 347, 384, 495, 643, 784...</td>\n",
       "      <td>mk.keyword_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cast_info</td>\n",
       "      <td>person_id</td>\n",
       "      <td>[0.0, 0.0, 5.518102507748589e-08, 2.7590512538...</td>\n",
       "      <td>[2, 77446, 145798, 212750, 281691, 347240, 419...</td>\n",
       "      <td>ci.person_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>movie_info_idx</td>\n",
       "      <td>info_type_id</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 9...</td>\n",
       "      <td>mi_idx.info_type_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>movie_info</td>\n",
       "      <td>info_type_id</td>\n",
       "      <td>[0.0, 0.05406815807174563, 0.08688004942665738...</td>\n",
       "      <td>[1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, ...</td>\n",
       "      <td>mi.info_type_id</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             table           column  \\\n",
       "0            title  production_year   \n",
       "1            title          kind_id   \n",
       "2  movie_companies       company_id   \n",
       "3  movie_companies  company_type_id   \n",
       "4        cast_info          role_id   \n",
       "5    movie_keyword       keyword_id   \n",
       "6        cast_info        person_id   \n",
       "7   movie_info_idx     info_type_id   \n",
       "8       movie_info     info_type_id   \n",
       "\n",
       "                                                freq  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.26216118191156074, 0.03593387047716835...   \n",
       "2  [0.0, 0.0004959511376981121, 0.000558807386989...   \n",
       "3      [0.0, 0.4883796425472418, 0.5116203574527581]   \n",
       "4  [0.0, 0.3495907485479872, 0.20560375449487386,...   \n",
       "5  [0.0, 0.0031748950967179193, 1.989421142551088...   \n",
       "6  [0.0, 0.0, 5.518102507748589e-08, 2.7590512538...   \n",
       "7  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "8  [0.0, 0.05406815807174563, 0.08688004942665738...   \n",
       "\n",
       "                                                bins         table_column  \n",
       "0  [1880, 1913, 1923, 1942, 1955, 1960, 1964, 196...    t.production_year  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, ...            t.kind_id  \n",
       "2  [1, 6, 19, 27, 68, 133, 160, 189, 292, 402, 47...        mc.company_id  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   mc.company_type_id  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...           ci.role_id  \n",
       "5  [1, 77, 132, 230, 331, 347, 384, 495, 643, 784...        mk.keyword_id  \n",
       "6  [2, 77446, 145798, 212750, 281691, 347240, 419...         ci.person_id  \n",
       "7  [99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 9...  mi_idx.info_type_id  \n",
       "8  [1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, ...      mi.info_type_id  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_file.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf8a50c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "table           object\n",
       "column          object\n",
       "freq            object\n",
       "bins            object\n",
       "table_column    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_file.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "567edfc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: table\n",
      "table\n",
      "<class 'str'>    9\n",
      "Name: count, dtype: int64\n",
      "Sample Value: title\n",
      "\n",
      "\n",
      "Column: column\n",
      "column\n",
      "<class 'str'>    9\n",
      "Name: count, dtype: int64\n",
      "Sample Value: production_year\n",
      "\n",
      "\n",
      "Column: freq\n",
      "freq\n",
      "<class 'numpy.ndarray'>    9\n",
      "Name: count, dtype: int64\n",
      "Sample Value: [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 1.22139046e-06\n",
      " 0.00000000e+00 8.14260304e-07]\n",
      "Length of Sample Value: 2020\n",
      "\n",
      "\n",
      "Column: bins\n",
      "bins\n",
      "<class 'list'>    9\n",
      "Name: count, dtype: int64\n",
      "Sample Value: [1880, 1913, 1923, 1942, 1955, 1960, 1964, 1968, 1971, 1975, 1978, 1982, 1985, 1987, 1990, 1992, 1994, 1995, 1996, 1998, 1999, 2000, 2001, 2001, 2002, 2003, 2004, 2004, 2005, 2005, 2006, 2006, 2007, 2007, 2007, 2008, 2008, 2009, 2009, 2009, 2010, 2010, 2010, 2011, 2011, 2011, 2012, 2012, 2012, 2013, 2019]\n",
      "Length of Sample Value: 51\n",
      "\n",
      "\n",
      "Column: table_column\n",
      "table_column\n",
      "<class 'str'>    9\n",
      "Name: count, dtype: int64\n",
      "Sample Value: t.production_year\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for column in ['table', 'column', 'freq', 'bins', 'table_column']:\n",
    "    print(f\"Column: {column}\")\n",
    "    print(hist_file[column].apply(type).value_counts())\n",
    "    sample_value = hist_file[column].iloc[0]\n",
    "    print(f\"Sample Value: {sample_value}\")\n",
    "    if isinstance(sample_value, (list, tuple, set, dict, pd.Series, np.ndarray)):\n",
    "        print(f\"Length of Sample Value: {len(sample_value)}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a257143c",
   "metadata": {},
   "source": [
    "# Step 1: Identifying the training dataset and its component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98d193b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = PlanTreeDataset(full_train_df, None, encoding, hist_file, card_norm, cost_norm, to_predict, table_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eb32ba",
   "metadata": {},
   "source": [
    "# Step 2: Exploring full_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "506ced46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(full_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7c0c0aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17ae5bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>{\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>{\"Plan\": {\"Node Type\": \"Seq Scan\", \"Parallel A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               json\n",
       "0   0  {\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa...\n",
       "1   1  {\"Plan\": {\"Node Type\": \"Seq Scan\", \"Parallel A..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc0ead2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id       int64\n",
       "json    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2c2b1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Aware\": false, \"Startup Cost\": 23540.58, \"Total Cost\": 154548.95, \"Plan Rows\": 567655, \"Plan Width\": 119, \"Actual Startup Time\": 386.847, \"Actual Total Time\": 646.972, \"Actual Rows\": 283812, \"Actual Loops\": 1, \"Workers Planned\": 2, \"Workers Launched\": 2, \"Single Copy\": false, \"Plans\": [{\"Node Type\": \"Hash Join\", \"Parent Relationship\": \"Outer\", \"Parallel Aware\": true, \"Join Type\": \"Inner\", \"Startup Cost\": 22540.58, \"Total Cost\": 96783.45, \"Plan Rows\": 236523, \"Plan Width\": 119, \"Actual Startup Time\": 369.985, \"Actual Total Time\": 518.487, \"Actual Rows\": 94604, \"Actual Loops\": 3, \"Inner Unique\": false, \"Hash Cond\": \"(t.id = mi_idx.movie_id)\", \"Workers\": [], \"Plans\": [{\"Node Type\": \"Seq Scan\", \"Parent Relationship\": \"Outer\", \"Parallel Aware\": true, \"Relation Name\": \"title\", \"Alias\": \"t\", \"Startup Cost\": 0.0, \"Total Cost\": 49166.46, \"Plan Rows\": 649574, \"Plan Width\": 94, \"Actual Startup Time\": 0.366, \"Actual Total Time\": 147.047, \"Actual Rows\": 514421, \"Actual Loops\": 3, \"Filter\": \"(kind_id = 7)\", \"Rows Removed by Filter\": 328349, \"Workers\": []}, {\"Node Type\": \"Hash\", \"Parent Relationship\": \"Inner\", \"Parallel Aware\": true, \"Startup Cost\": 15122.68, \"Total Cost\": 15122.68, \"Plan Rows\": 383592, \"Plan Width\": 25, \"Actual Startup Time\": 103.547, \"Actual Total Time\": 103.547, \"Actual Rows\": 306703, \"Actual Loops\": 3, \"Hash Buckets\": 65536, \"Original Hash Buckets\": 65536, \"Hash Batches\": 32, \"Original Hash Batches\": 32, \"Peak Memory Usage\": 1920, \"Workers\": [], \"Plans\": [{\"Node Type\": \"Seq Scan\", \"Parent Relationship\": \"Outer\", \"Parallel Aware\": true, \"Relation Name\": \"movie_info_idx\", \"Alias\": \"mi_idx\", \"Startup Cost\": 0.0, \"Total Cost\": 15122.68, \"Plan Rows\": 383592, \"Plan Width\": 25, \"Actual Startup Time\": 0.28, \"Actual Total Time\": 54.382, \"Actual Rows\": 306703, \"Actual Loops\": 3, \"Filter\": \"(info_type_id > 99)\", \"Rows Removed by Filter\": 153308, \"Workers\": []}]}]}]}, \"Planning Time\": 2.382, \"Triggers\": [], \"Execution Time\": 654.241}\n"
     ]
    }
   ],
   "source": [
    "# printing a sample json string in full\n",
    "json_sample = full_train_df['json'].iloc[0]\n",
    "print(json_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d31b0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Plan\": {\n",
      "        \"Node Type\": \"Gather\",\n",
      "        \"Parallel Aware\": false,\n",
      "        \"Startup Cost\": 23540.58,\n",
      "        \"Total Cost\": 154548.95,\n",
      "        \"Plan Rows\": 567655,\n",
      "        \"Plan Width\": 119,\n",
      "        \"Actual Startup Time\": 386.847,\n",
      "        \"Actual Total Time\": 646.972,\n",
      "        \"Actual Rows\": 283812,\n",
      "        \"Actual Loops\": 1,\n",
      "        \"Workers Planned\": 2,\n",
      "        \"Workers Launched\": 2,\n",
      "        \"Single Copy\": false,\n",
      "        \"Plans\": [\n",
      "            {\n",
      "                \"Node Type\": \"Hash Join\",\n",
      "                \"Parent Relationship\": \"Outer\",\n",
      "                \"Parallel Aware\": true,\n",
      "                \"Join Type\": \"Inner\",\n",
      "                \"Startup Cost\": 22540.58,\n",
      "                \"Total Cost\": 96783.45,\n",
      "                \"Plan Rows\": 236523,\n",
      "                \"Plan Width\": 119,\n",
      "                \"Actual Startup Time\": 369.985,\n",
      "                \"Actual Total Time\": 518.487,\n",
      "                \"Actual Rows\": 94604,\n",
      "                \"Actual Loops\": 3,\n",
      "                \"Inner Unique\": false,\n",
      "                \"Hash Cond\": \"(t.id = mi_idx.movie_id)\",\n",
      "                \"Workers\": [],\n",
      "                \"Plans\": [\n",
      "                    {\n",
      "                        \"Node Type\": \"Seq Scan\",\n",
      "                        \"Parent Relationship\": \"Outer\",\n",
      "                        \"Parallel Aware\": true,\n",
      "                        \"Relation Name\": \"title\",\n",
      "                        \"Alias\": \"t\",\n",
      "                        \"Startup Cost\": 0.0,\n",
      "                        \"Total Cost\": 49166.46,\n",
      "                        \"Plan Rows\": 649574,\n",
      "                        \"Plan Width\": 94,\n",
      "                        \"Actual Startup Time\": 0.366,\n",
      "                        \"Actual Total Time\": 147.047,\n",
      "                        \"Actual Rows\": 514421,\n",
      "                        \"Actual Loops\": 3,\n",
      "                        \"Filter\": \"(kind_id = 7)\",\n",
      "                        \"Rows Removed by Filter\": 328349,\n",
      "                        \"Workers\": []\n",
      "                    },\n",
      "                    {\n",
      "                        \"Node Type\": \"Hash\",\n",
      "                        \"Parent Relationship\": \"Inner\",\n",
      "                        \"Parallel Aware\": true,\n",
      "                        \"Startup Cost\": 15122.68,\n",
      "                        \"Total Cost\": 15122.68,\n",
      "                        \"Plan Rows\": 383592,\n",
      "                        \"Plan Width\": 25,\n",
      "                        \"Actual Startup Time\": 103.547,\n",
      "                        \"Actual Total Time\": 103.547,\n",
      "                        \"Actual Rows\": 306703,\n",
      "                        \"Actual Loops\": 3,\n",
      "                        \"Hash Buckets\": 65536,\n",
      "                        \"Original Hash Buckets\": 65536,\n",
      "                        \"Hash Batches\": 32,\n",
      "                        \"Original Hash Batches\": 32,\n",
      "                        \"Peak Memory Usage\": 1920,\n",
      "                        \"Workers\": [],\n",
      "                        \"Plans\": [\n",
      "                            {\n",
      "                                \"Node Type\": \"Seq Scan\",\n",
      "                                \"Parent Relationship\": \"Outer\",\n",
      "                                \"Parallel Aware\": true,\n",
      "                                \"Relation Name\": \"movie_info_idx\",\n",
      "                                \"Alias\": \"mi_idx\",\n",
      "                                \"Startup Cost\": 0.0,\n",
      "                                \"Total Cost\": 15122.68,\n",
      "                                \"Plan Rows\": 383592,\n",
      "                                \"Plan Width\": 25,\n",
      "                                \"Actual Startup Time\": 0.28,\n",
      "                                \"Actual Total Time\": 54.382,\n",
      "                                \"Actual Rows\": 306703,\n",
      "                                \"Actual Loops\": 3,\n",
      "                                \"Filter\": \"(info_type_id > 99)\",\n",
      "                                \"Rows Removed by Filter\": 153308,\n",
      "                                \"Workers\": []\n",
      "                            }\n",
      "                        ]\n",
      "                    }\n",
      "                ]\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"Planning Time\": 2.382,\n",
      "    \"Triggers\": [],\n",
      "    \"Execution Time\": 654.241\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# pretty print the json object\n",
    "import json\n",
    "# the following code parses the json string into a dictionary\n",
    "json_parsed = json.loads(json_sample)\n",
    "json_pretty = json.dumps(json_parsed, indent=4)\n",
    "print(json_pretty)\n",
    "\n",
    "with open('output.json', 'w') as f:\n",
    "    f.write(json_pretty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1148484f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model.database_util.Encoding"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0dba5659",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = PlanTreeDataset(val_df, None, encoding, hist_file, card_norm, cost_norm, to_predict, table_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdcefceb",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 46.00 MiB. GPU 0 has a total capacity of 3.94 GiB of which 3.06 MiB is free. Including non-PyTorch memory, this process has 3.91 GiB memory in use. Of the allocated memory 3.70 GiB is allocated by PyTorch, and 146.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m crit \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[0;32m----> 2\u001b[0m model, best_path \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcost_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/QueryFormer/model/trainer.py:125\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_ds, val_ds, crit, cost_norm, args, optimizer, scheduler)\u001b[0m\n\u001b[1;32m    122\u001b[0m batch_cost_label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(l)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    123\u001b[0m batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 125\u001b[0m cost_preds, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m cost_preds \u001b[38;5;241m=\u001b[39m cost_preds\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    128\u001b[0m loss \u001b[38;5;241m=\u001b[39m crit(cost_preds, batch_cost_label)\n",
      "File \u001b[0;32m~/QueryFormer/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/QueryFormer/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/QueryFormer/model/model.py:225\u001b[0m, in \u001b[0;36mQueryFormer.forward\u001b[0;34m(self, batched_data)\u001b[0m\n\u001b[1;32m    223\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dropout(super_node_feature)\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m enc_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 225\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43menc_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree_attn_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_ln(output)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred(output[:,\u001b[38;5;241m0\u001b[39m,:]), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred2(output[:,\u001b[38;5;241m0\u001b[39m,:])\n",
      "File \u001b[0;32m~/QueryFormer/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/QueryFormer/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/QueryFormer/model/model.py:315\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[0;34m(self, x, attn_bias)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, attn_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    314\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attention_norm(x)\n\u001b[0;32m--> 315\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attention_dropout(y)\n\u001b[1;32m    317\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m y\n",
      "File \u001b[0;32m~/QueryFormer/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/QueryFormer/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/QueryFormer/model/model.py:286\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, q, k, v, attn_bias)\u001b[0m\n\u001b[1;32m    284\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(q, k)  \u001b[38;5;66;03m# [b, h, q_len, k_len]\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 286\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattn_bias\u001b[49m\n\u001b[1;32m    288\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    289\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matt_dropout(x)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 46.00 MiB. GPU 0 has a total capacity of 3.94 GiB of which 3.06 MiB is free. Including non-PyTorch memory, this process has 3.91 GiB memory in use. Of the allocated memory 3.70 GiB is allocated by PyTorch, and 146.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "crit = nn.MSELoss()\n",
    "model, best_path = train(model, train_ds, val_ds, crit, cost_norm, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dc344e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f1095a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa517f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = {\n",
    "    'get_sample' : get_job_table_sample,\n",
    "    'encoding': encoding,\n",
    "    'cost_norm': cost_norm,\n",
    "    'hist_file': hist_file,\n",
    "    'model': model,\n",
    "    'device': args.device,\n",
    "    'bs': 512,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2b14a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9e7796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd89df96",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = eval_workload('job-light', methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e40c30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = eval_workload('synthetic', methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30aceed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e47dfb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0622ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b92aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ceb39d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
