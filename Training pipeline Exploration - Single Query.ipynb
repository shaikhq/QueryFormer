{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a13f129",
   "metadata": {},
   "source": [
    "# Importing Python Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9c5213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "037bec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.util import Normalizer\n",
    "from model.database_util import get_hist_file, get_job_table_sample, collator\n",
    "from model.model import QueryFormer\n",
    "from model.database_util import Encoding\n",
    "from model.dataset import PlanTreeDataset\n",
    "from model.trainer import eval_workload, train, train_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "822fdcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/imdb/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900417b1",
   "metadata": {},
   "source": [
    "# DL Model's Hyper parameters definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbcd4773",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    # bs = 1024\n",
    "    # SQ: smaller batch size\n",
    "    bs = 1\n",
    "    lr = 0.001\n",
    "    # epochs = 200\n",
    "    epochs = 1\n",
    "    clip_size = 50\n",
    "    embed_size = 64\n",
    "    pred_hid = 128\n",
    "    ffn_dim = 128\n",
    "    head_size = 12\n",
    "    n_layers = 8\n",
    "    dropout = 0.1\n",
    "    sch_decay = 0.6\n",
    "    # device = 'cuda:0'\n",
    "    device = 'cpu'\n",
    "    newpath = './results/full/cost/'\n",
    "    to_predict = 'card'\n",
    "args = Args()\n",
    "\n",
    "import os\n",
    "if not os.path.exists(args.newpath):\n",
    "    os.makedirs(args.newpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c33199",
   "metadata": {},
   "source": [
    "# Defining Normalizing functions for card\n",
    "1. Is Normalizer a function defined by the authors of QueryFormer?\n",
    "Yes, it's a class. \n",
    "\n",
    "2. Where do the following values for the normalizer come from?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51743c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cost_norm = Normalizer(-3.61192, 12.290855)\n",
    "card_norm = Normalizer(1,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8258bcf9",
   "metadata": {},
   "source": [
    "# Loading Encoding file\n",
    "1. Who created this encoding file in the first place?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e5f421a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_ckpt = torch.load('checkpoints/encoding.pt')\n",
    "type(encoding_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adbb38f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['encoding'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_ckpt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb839f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model.database_util.Encoding"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = encoding_ckpt['encoding']\n",
    "type(encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3335b172",
   "metadata": {},
   "source": [
    "## Exploring Encoding object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "086ed48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'t.id': [1.0, 2528312.0], 't.kind_id': [1.0, 7.0], 't.production_year': [1880.0, 2019.0], 'mc.id': [1.0, 2609129.0], 'mc.company_id': [1.0, 234997.0], 'mc.movie_id': [2.0, 2525745.0], 'mc.company_type_id': [1.0, 2.0], 'ci.id': [1.0, 36244344.0], 'ci.movie_id': [1.0, 2525975.0], 'ci.person_id': [1.0, 4061926.0], 'ci.role_id': [1.0, 11.0], 'mi.id': [1.0, 14835720.0], 'mi.movie_id': [1.0, 2526430.0], 'mi.info_type_id': [1.0, 110.0], 'mi_idx.id': [1.0, 1380035.0], 'mi_idx.movie_id': [2.0, 2525793.0], 'mi_idx.info_type_id': [99.0, 113.0], 'mk.id': [1.0, 4523930.0], 'mk.movie_id': [2.0, 2525971.0], 'mk.keyword_id': [1.0, 134170.0]}\n"
     ]
    }
   ],
   "source": [
    "print(encoding.column_min_max_vals)\n",
    "# column_min_max_vals is a dictionary. It has the min and max value for each numeric column in the dataset. \n",
    "# Q. how is column min and max used?\n",
    "# Q. All these keys, for which min and max are provided, are categorical features. What's the use of these columns' min and max? (if they were numeric columns, gathering their min and max would have made sense.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d42de580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'t.id': 0, 't.kind_id': 1, 't.production_year': 2, 'mc.id': 3, 'mc.company_id': 4, 'mc.movie_id': 5, 'mc.company_type_id': 6, 'ci.id': 7, 'ci.movie_id': 8, 'ci.person_id': 9, 'ci.role_id': 10, 'mi.id': 11, 'mi.movie_id': 12, 'mi.info_type_id': 13, 'mi_idx.id': 14, 'mi_idx.movie_id': 15, 'mi_idx.info_type_id': 16, 'mk.id': 17, 'mk.movie_id': 18, 'mk.keyword_id': 19, 'NA': 20}\n"
     ]
    }
   ],
   "source": [
    "print(encoding.col2idx)\n",
    "# the label encoding of each unique column in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8bd229d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'>': 0, '=': 1, '<': 2, 'NA': 3}\n"
     ]
    }
   ],
   "source": [
    "print(encoding.op2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5d23a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 't.id', 1: 't.kind_id', 2: 't.production_year', 3: 'mc.id', 4: 'mc.company_id', 5: 'mc.movie_id', 6: 'mc.company_type_id', 7: 'ci.id', 8: 'ci.movie_id', 9: 'ci.person_id', 10: 'ci.role_id', 11: 'mi.id', 12: 'mi.movie_id', 13: 'mi.info_type_id', 14: 'mi_idx.id', 15: 'mi_idx.movie_id', 16: 'mi_idx.info_type_id', 17: 'mk.id', 18: 'mk.movie_id', 19: 'mk.keyword_id', 20: 'NA'}\n"
     ]
    }
   ],
   "source": [
    "print(encoding.idx2col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cec6d6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gather': 0, 'Hash Join': 1, 'Seq Scan': 2, 'Hash': 3, 'Bitmap Heap Scan': 4, 'Bitmap Index Scan': 5, 'Nested Loop': 6, 'Index Scan': 7, 'Merge Join': 8, 'Gather Merge': 9, 'Materialize': 10, 'BitmapAnd': 11, 'Sort': 12}\n"
     ]
    }
   ],
   "source": [
    "print(encoding.type2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b660cb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('checkpoints/cost_model.pt', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71759b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.util import seed_everything\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9592f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QueryFormer(emb_size = args.embed_size ,ffn_dim = args.ffn_dim, head_size = args.head_size, \\\n",
    "                 dropout = args.dropout, n_layers = args.n_layers, \\\n",
    "                 use_sample = True, use_hist = True, \\\n",
    "                 pred_hid = args.pred_hid\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85e27584",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c8a7d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict = 'cost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4286ce8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_path = './data/imdb/'\n",
    "dfs = []  # list to hold DataFrames\n",
    "# SQ: added\n",
    "#for i in range(2):\n",
    "for i in range(18):\n",
    "    file = imdb_path + 'plan_and_cost/train_plan_part{}.csv'.format(i)\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "full_train_df = pd.concat(dfs)\n",
    "\n",
    "val_dfs = []  # list to hold DataFrames\n",
    "for i in range(18,20):\n",
    "    file = imdb_path + 'plan_and_cost/train_plan_part{}.csv'.format(i)\n",
    "    df = pd.read_csv(file)\n",
    "    val_dfs.append(df)\n",
    "\n",
    "val_df = pd.concat(val_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54150b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90000, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "030b6fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd8301be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded queries with len  100000\n",
      "Loaded bitmaps\n"
     ]
    }
   ],
   "source": [
    "table_sample = get_job_table_sample(imdb_path+'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c0f3797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(table_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e14309fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(table_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca1dd3ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(table_sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2bd5d5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['title', 'movie_info_idx'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_sample[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45199c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_sample[0]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4bea339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>{\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               json\n",
       "0   0  {\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab721a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90000</td>\n",
       "      <td>{\"Plan\": {\"Node Type\": \"Nested Loop\", \"Paralle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               json\n",
       "0  90000  {\"Plan\": {\"Node Type\": \"Nested Loop\", \"Paralle..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f06ae89",
   "metadata": {},
   "source": [
    "## Loading and Exploring the Histogram (hist_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b9e9d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shaikhq/coding/QueryFormer/model/database_util.py:76: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  hist_file['freq'][i] = freq_np\n",
      "/Users/shaikhq/coding/QueryFormer/model/database_util.py:89: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  hist_file['bins'][rid] = \\\n",
      "/Users/shaikhq/coding/QueryFormer/model/database_util.py:89: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  hist_file['bins'][rid] = \\\n",
      "/Users/shaikhq/coding/QueryFormer/model/database_util.py:89: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  hist_file['bins'][rid] = \\\n",
      "/Users/shaikhq/coding/QueryFormer/model/database_util.py:89: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  hist_file['bins'][rid] = \\\n",
      "/Users/shaikhq/coding/QueryFormer/model/database_util.py:89: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  hist_file['bins'][rid] = \\\n",
      "/Users/shaikhq/coding/QueryFormer/model/database_util.py:89: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  hist_file['bins'][rid] = \\\n",
      "/Users/shaikhq/coding/QueryFormer/model/database_util.py:89: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  hist_file['bins'][rid] = \\\n",
      "/Users/shaikhq/coding/QueryFormer/model/database_util.py:89: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  hist_file['bins'][rid] = \\\n",
      "/Users/shaikhq/coding/QueryFormer/model/database_util.py:89: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  hist_file['bins'][rid] = \\\n"
     ]
    }
   ],
   "source": [
    "hist_file = get_hist_file(data_path + 'histogram_string.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f377f08c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(hist_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e108ba9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 5)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f590fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table</th>\n",
       "      <th>column</th>\n",
       "      <th>freq</th>\n",
       "      <th>bins</th>\n",
       "      <th>table_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>title</td>\n",
       "      <td>production_year</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[1880, 1913, 1923, 1942, 1955, 1960, 1964, 196...</td>\n",
       "      <td>t.production_year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>title</td>\n",
       "      <td>kind_id</td>\n",
       "      <td>[0.0, 0.26216118191156074, 0.03593387047716835...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, ...</td>\n",
       "      <td>t.kind_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>movie_companies</td>\n",
       "      <td>company_id</td>\n",
       "      <td>[0.0, 0.0004959511376981121, 0.000558807386989...</td>\n",
       "      <td>[1, 6, 19, 27, 68, 133, 160, 189, 292, 402, 47...</td>\n",
       "      <td>mc.company_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>movie_companies</td>\n",
       "      <td>company_type_id</td>\n",
       "      <td>[0.0, 0.4883796425472418, 0.5116203574527581]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>mc.company_type_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cast_info</td>\n",
       "      <td>role_id</td>\n",
       "      <td>[0.0, 0.3495907485479872, 0.20560375449487386,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>ci.role_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>movie_keyword</td>\n",
       "      <td>keyword_id</td>\n",
       "      <td>[0.0, 0.0031748950967179193, 1.989421142551088...</td>\n",
       "      <td>[1, 77, 132, 230, 331, 347, 384, 495, 643, 784...</td>\n",
       "      <td>mk.keyword_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cast_info</td>\n",
       "      <td>person_id</td>\n",
       "      <td>[0.0, 0.0, 5.518102507748589e-08, 2.7590512538...</td>\n",
       "      <td>[2, 77446, 145798, 212750, 281691, 347240, 419...</td>\n",
       "      <td>ci.person_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>movie_info_idx</td>\n",
       "      <td>info_type_id</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 9...</td>\n",
       "      <td>mi_idx.info_type_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>movie_info</td>\n",
       "      <td>info_type_id</td>\n",
       "      <td>[0.0, 0.05406815807174563, 0.08688004942665738...</td>\n",
       "      <td>[1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, ...</td>\n",
       "      <td>mi.info_type_id</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             table           column  \\\n",
       "0            title  production_year   \n",
       "1            title          kind_id   \n",
       "2  movie_companies       company_id   \n",
       "3  movie_companies  company_type_id   \n",
       "4        cast_info          role_id   \n",
       "5    movie_keyword       keyword_id   \n",
       "6        cast_info        person_id   \n",
       "7   movie_info_idx     info_type_id   \n",
       "8       movie_info     info_type_id   \n",
       "\n",
       "                                                freq  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.26216118191156074, 0.03593387047716835...   \n",
       "2  [0.0, 0.0004959511376981121, 0.000558807386989...   \n",
       "3      [0.0, 0.4883796425472418, 0.5116203574527581]   \n",
       "4  [0.0, 0.3495907485479872, 0.20560375449487386,...   \n",
       "5  [0.0, 0.0031748950967179193, 1.989421142551088...   \n",
       "6  [0.0, 0.0, 5.518102507748589e-08, 2.7590512538...   \n",
       "7  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "8  [0.0, 0.05406815807174563, 0.08688004942665738...   \n",
       "\n",
       "                                                bins         table_column  \n",
       "0  [1880, 1913, 1923, 1942, 1955, 1960, 1964, 196...    t.production_year  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, ...            t.kind_id  \n",
       "2  [1, 6, 19, 27, 68, 133, 160, 189, 292, 402, 47...        mc.company_id  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   mc.company_type_id  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...           ci.role_id  \n",
       "5  [1, 77, 132, 230, 331, 347, 384, 495, 643, 784...        mk.keyword_id  \n",
       "6  [2, 77446, 145798, 212750, 281691, 347240, 419...         ci.person_id  \n",
       "7  [99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 9...  mi_idx.info_type_id  \n",
       "8  [1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, ...      mi.info_type_id  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_file.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf8a50c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "table           object\n",
       "column          object\n",
       "freq            object\n",
       "bins            object\n",
       "table_column    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_file.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "567edfc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: table\n",
      "table\n",
      "<class 'str'>    9\n",
      "Name: count, dtype: int64\n",
      "Sample Value: title\n",
      "\n",
      "\n",
      "Column: column\n",
      "column\n",
      "<class 'str'>    9\n",
      "Name: count, dtype: int64\n",
      "Sample Value: production_year\n",
      "\n",
      "\n",
      "Column: freq\n",
      "freq\n",
      "<class 'numpy.ndarray'>    9\n",
      "Name: count, dtype: int64\n",
      "Sample Value: [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 1.22139046e-06\n",
      " 0.00000000e+00 8.14260304e-07]\n",
      "Length of Sample Value: 2020\n",
      "\n",
      "\n",
      "Column: bins\n",
      "bins\n",
      "<class 'list'>    9\n",
      "Name: count, dtype: int64\n",
      "Sample Value: [1880, 1913, 1923, 1942, 1955, 1960, 1964, 1968, 1971, 1975, 1978, 1982, 1985, 1987, 1990, 1992, 1994, 1995, 1996, 1998, 1999, 2000, 2001, 2001, 2002, 2003, 2004, 2004, 2005, 2005, 2006, 2006, 2007, 2007, 2007, 2008, 2008, 2009, 2009, 2009, 2010, 2010, 2010, 2011, 2011, 2011, 2012, 2012, 2012, 2013, 2019]\n",
      "Length of Sample Value: 51\n",
      "\n",
      "\n",
      "Column: table_column\n",
      "table_column\n",
      "<class 'str'>    9\n",
      "Name: count, dtype: int64\n",
      "Sample Value: t.production_year\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for column in ['table', 'column', 'freq', 'bins', 'table_column']:\n",
    "    print(f\"Column: {column}\")\n",
    "    print(hist_file[column].apply(type).value_counts())\n",
    "    sample_value = hist_file[column].iloc[0]\n",
    "    print(f\"Sample Value: {sample_value}\")\n",
    "    if isinstance(sample_value, (list, tuple, set, dict, pd.Series, np.ndarray)):\n",
    "        print(f\"Length of Sample Value: {len(sample_value)}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "755fb48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table</th>\n",
       "      <th>column</th>\n",
       "      <th>freq</th>\n",
       "      <th>bins</th>\n",
       "      <th>table_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>title</td>\n",
       "      <td>production_year</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[1880, 1913, 1923, 1942, 1955, 1960, 1964, 196...</td>\n",
       "      <td>t.production_year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>title</td>\n",
       "      <td>kind_id</td>\n",
       "      <td>[0.0, 0.26216118191156074, 0.03593387047716835...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, ...</td>\n",
       "      <td>t.kind_id</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   table           column                                               freq  \\\n",
       "0  title  production_year  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  title          kind_id  [0.0, 0.26216118191156074, 0.03593387047716835...   \n",
       "\n",
       "                                                bins       table_column  \n",
       "0  [1880, 1913, 1923, 1942, 1955, 1960, 1964, 196...  t.production_year  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, ...          t.kind_id  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking into only the columns from the same table\n",
    "hist_file_title = hist_file[hist_file['table'] == 'title']\n",
    "hist_file_title.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8513477d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "table           object\n",
       "column          object\n",
       "freq            object\n",
       "bins            object\n",
       "table_column    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the length of bins for the columns in the title table\n",
    "hist_file_title.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a6345a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing the type of object in the bins column\n",
    "type(hist_file_title['bins'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "311feeed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing the length of the list, represenging the bin, for the production year column\n",
    "len(hist_file_title['bins'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "786ff539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing the number of bins for the kind_id column. I want to check if the number of bins are same for all columns from a same table, e.g., title. \n",
    "len(hist_file_title['bins'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e35e723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: title, Column: production_year, Length of freq: 2020, Length of bins: 51\n",
      "Table: title, Column: kind_id, Length of freq: 8, Length of bins: 51\n",
      "Table: movie_companies, Column: company_id, Length of freq: 234998, Length of bins: 51\n",
      "Table: movie_companies, Column: company_type_id, Length of freq: 3, Length of bins: 51\n",
      "Table: cast_info, Column: role_id, Length of freq: 12, Length of bins: 51\n",
      "Table: movie_keyword, Column: keyword_id, Length of freq: 134171, Length of bins: 51\n",
      "Table: cast_info, Column: person_id, Length of freq: 4061927, Length of bins: 51\n",
      "Table: movie_info_idx, Column: info_type_id, Length of freq: 114, Length of bins: 51\n",
      "Table: movie_info, Column: info_type_id, Length of freq: 111, Length of bins: 51\n"
     ]
    }
   ],
   "source": [
    "# now print the length of bins from all rows, across multiple tables. I expect the number of bins to be equal for all columns within the same table, however, they may differ across tables. Let's see. \n",
    "for index, row in hist_file.iterrows():\n",
    "    print(f\"Table: {row['table']}, Column: {row['column']}, Length of freq: {len(row['freq'])}, Length of bins: {len(row['bins'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2b4177db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freq for kind_id:\n",
      "[0.         0.26216118 0.03593387 0.03976449 0.04676403 0.\n",
      " 0.00498356 0.61039287]\n",
      "\n",
      "Bins for kind_id:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 3, 4, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n"
     ]
    }
   ],
   "source": [
    "# let's print the content of the freq and bins column for kind_id\n",
    "kind_id_row = hist_file[hist_file['column'] == 'kind_id'].iloc[0]\n",
    "\n",
    "print(\"Freq for kind_id:\")\n",
    "print(kind_id_row['freq'])\n",
    "\n",
    "print(\"\\nBins for kind_id:\")\n",
    "print(kind_id_row['bins'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "64e82eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct bins for kind_id:\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# I think the number of values in the freq column is equal to the number of origin bins in a column, not the re-calibrated bins to match the number of bins across columns in the same table. Let's check. \n",
    "kind_id_bins = hist_file[hist_file['column'] == 'kind_id'].iloc[0]['bins']\n",
    "\n",
    "distinct_bins = set(kind_id_bins)\n",
    "\n",
    "print(\"Distinct bins for kind_id:\")\n",
    "for bin in distinct_bins:\n",
    "    print(bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "37d356b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freq for production_year:\n",
      "[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 1.22139046e-06\n",
      " 0.00000000e+00 8.14260304e-07]\n",
      "\n",
      "Bins for production_year:\n",
      "[1880, 1913, 1923, 1942, 1955, 1960, 1964, 1968, 1971, 1975, 1978, 1982, 1985, 1987, 1990, 1992, 1994, 1995, 1996, 1998, 1999, 2000, 2001, 2001, 2002, 2003, 2004, 2004, 2005, 2005, 2006, 2006, 2007, 2007, 2007, 2008, 2008, 2009, 2009, 2009, 2010, 2010, 2010, 2011, 2011, 2011, 2012, 2012, 2012, 2013, 2019]\n"
     ]
    }
   ],
   "source": [
    "# let's print the content of the freq and bins column for production_year\n",
    "production_year_row = hist_file[hist_file['column'] == 'production_year'].iloc[0]\n",
    "\n",
    "print(\"Freq for production_year:\")\n",
    "print(production_year_row['freq'])\n",
    "\n",
    "print(\"\\nBins for production_year:\")\n",
    "print(production_year_row['bins'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a257143c",
   "metadata": {},
   "source": [
    "# Step 1: Identifying the training dataset and its component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c4010929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model.util.Normalizer"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(card_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f755b9b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<model.util.Normalizer at 0x2c0afa600>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "card_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "04b52ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<model.util.Normalizer at 0x10518c5f0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "83f7652b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9fa33cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cost'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "38c6e0b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(table_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "91d8dafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90000"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bfa998d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(table_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "51d67299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(table_sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1607673d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8),\n",
       " 'movie_info_idx': array([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,\n",
       "        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,\n",
       "        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,\n",
       "        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
       "        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,\n",
       "        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 0, 0, 1, 0, 1, 1], dtype=uint8)}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fd1b70ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(table_sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "03c7250b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(table_sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dbca255d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['title', 'movie_info_idx'])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_sample[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "646e6144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['title'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_sample[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4572ca09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90000, 2)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29683b3f",
   "metadata": {},
   "source": [
    "# exploring the execution of the __init__ function of the PlanTreeDataset for 1 training query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c2d8cd45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90000, 2)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e9611dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_single_query = full_train_df.iloc[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d00660be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_single_query.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d7b7836f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>{\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               json\n",
       "0   0  {\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa..."
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_single_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2b2592e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Plan\": {\n",
      "        \"Node Type\": \"Gather\",\n",
      "        \"Parallel Aware\": false,\n",
      "        \"Startup Cost\": 23540.58,\n",
      "        \"Total Cost\": 154548.95,\n",
      "        \"Plan Rows\": 567655,\n",
      "        \"Plan Width\": 119,\n",
      "        \"Actual Startup Time\": 386.847,\n",
      "        \"Actual Total Time\": 646.972,\n",
      "        \"Actual Rows\": 283812,\n",
      "        \"Actual Loops\": 1,\n",
      "        \"Workers Planned\": 2,\n",
      "        \"Workers Launched\": 2,\n",
      "        \"Single Copy\": false,\n",
      "        \"Plans\": [\n",
      "            {\n",
      "                \"Node Type\": \"Hash Join\",\n",
      "                \"Parent Relationship\": \"Outer\",\n",
      "                \"Parallel Aware\": true,\n",
      "                \"Join Type\": \"Inner\",\n",
      "                \"Startup Cost\": 22540.58,\n",
      "                \"Total Cost\": 96783.45,\n",
      "                \"Plan Rows\": 236523,\n",
      "                \"Plan Width\": 119,\n",
      "                \"Actual Startup Time\": 369.985,\n",
      "                \"Actual Total Time\": 518.487,\n",
      "                \"Actual Rows\": 94604,\n",
      "                \"Actual Loops\": 3,\n",
      "                \"Inner Unique\": false,\n",
      "                \"Hash Cond\": \"(t.id = mi_idx.movie_id)\",\n",
      "                \"Workers\": [],\n",
      "                \"Plans\": [\n",
      "                    {\n",
      "                        \"Node Type\": \"Seq Scan\",\n",
      "                        \"Parent Relationship\": \"Outer\",\n",
      "                        \"Parallel Aware\": true,\n",
      "                        \"Relation Name\": \"title\",\n",
      "                        \"Alias\": \"t\",\n",
      "                        \"Startup Cost\": 0.0,\n",
      "                        \"Total Cost\": 49166.46,\n",
      "                        \"Plan Rows\": 649574,\n",
      "                        \"Plan Width\": 94,\n",
      "                        \"Actual Startup Time\": 0.366,\n",
      "                        \"Actual Total Time\": 147.047,\n",
      "                        \"Actual Rows\": 514421,\n",
      "                        \"Actual Loops\": 3,\n",
      "                        \"Filter\": \"(kind_id = 7)\",\n",
      "                        \"Rows Removed by Filter\": 328349,\n",
      "                        \"Workers\": []\n",
      "                    },\n",
      "                    {\n",
      "                        \"Node Type\": \"Hash\",\n",
      "                        \"Parent Relationship\": \"Inner\",\n",
      "                        \"Parallel Aware\": true,\n",
      "                        \"Startup Cost\": 15122.68,\n",
      "                        \"Total Cost\": 15122.68,\n",
      "                        \"Plan Rows\": 383592,\n",
      "                        \"Plan Width\": 25,\n",
      "                        \"Actual Startup Time\": 103.547,\n",
      "                        \"Actual Total Time\": 103.547,\n",
      "                        \"Actual Rows\": 306703,\n",
      "                        \"Actual Loops\": 3,\n",
      "                        \"Hash Buckets\": 65536,\n",
      "                        \"Original Hash Buckets\": 65536,\n",
      "                        \"Hash Batches\": 32,\n",
      "                        \"Original Hash Batches\": 32,\n",
      "                        \"Peak Memory Usage\": 1920,\n",
      "                        \"Workers\": [],\n",
      "                        \"Plans\": [\n",
      "                            {\n",
      "                                \"Node Type\": \"Seq Scan\",\n",
      "                                \"Parent Relationship\": \"Outer\",\n",
      "                                \"Parallel Aware\": true,\n",
      "                                \"Relation Name\": \"movie_info_idx\",\n",
      "                                \"Alias\": \"mi_idx\",\n",
      "                                \"Startup Cost\": 0.0,\n",
      "                                \"Total Cost\": 15122.68,\n",
      "                                \"Plan Rows\": 383592,\n",
      "                                \"Plan Width\": 25,\n",
      "                                \"Actual Startup Time\": 0.28,\n",
      "                                \"Actual Total Time\": 54.382,\n",
      "                                \"Actual Rows\": 306703,\n",
      "                                \"Actual Loops\": 3,\n",
      "                                \"Filter\": \"(info_type_id > 99)\",\n",
      "                                \"Rows Removed by Filter\": 153308,\n",
      "                                \"Workers\": []\n",
      "                            }\n",
      "                        ]\n",
      "                    }\n",
      "                ]\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"Planning Time\": 2.382,\n",
      "    \"Triggers\": [],\n",
      "    \"Execution Time\": 654.241\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# the following code parses the json string into a dictionary\n",
    "json_parsed = json.loads(train_df_single_query['json'].iloc[0])\n",
    "json_pretty = json.dumps(json_parsed, indent=4)\n",
    "print(json_pretty)\n",
    "\n",
    "with open('output.json', 'w') as f:\n",
    "    f.write(json_pretty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "41a30532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--Gather\n",
      "   +--Hash Join\n",
      "   |  +--Seq Scan\n",
      "      +--Hash\n",
      "         +--Seq Scan\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def print_plan(plan, indent=''):\n",
    "    print(indent + '+--' + plan['Node Type'])\n",
    "    if 'Plans' in plan:\n",
    "        for i, subplan in enumerate(plan['Plans']):\n",
    "            if i == len(plan['Plans']) - 1:\n",
    "                new_indent = indent + '   '\n",
    "            else:\n",
    "                new_indent = indent + '|  '\n",
    "            print_plan(subplan, new_indent)\n",
    "\n",
    "# Load the execution plan from a string, file, etc.\n",
    "execution_plan = json.loads(train_df_single_query['json'].iloc[0])\n",
    "\n",
    "print_plan(execution_plan['Plan'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2852ecf4",
   "metadata": {},
   "source": [
    "The query is similar to the following: (generated by Github copilot)\n",
    "```sql\n",
    "SELECT *\n",
    "FROM title AS t\n",
    "JOIN movie_info_idx AS mi_idx ON t.id = mi_idx.movie_id\n",
    "WHERE t.kind_id = 7 AND mi_idx.info_type_id > 99;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e696a109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# import importlib\n",
    "\n",
    "# importlib.reload(model.dataset)\n",
    "# from model.dataset import PlanTreeDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d2f8b5fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cost'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f2c12fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Initializing PlanTreeDataset\n",
      "INFO:root:self.length = len(json_df): 1\n",
      "INFO:root:nodes.type: <class 'list'>\n",
      "INFO:root:number of nodes: 1\n",
      "INFO:root:type of the first element in the list nodes: <class 'dict'>\n",
      "INFO:root:keys in the first dictionary in the nodes list: dict_keys(['Node Type', 'Parallel Aware', 'Startup Cost', 'Total Cost', 'Plan Rows', 'Plan Width', 'Actual Startup Time', 'Actual Total Time', 'Actual Rows', 'Actual Loops', 'Workers Planned', 'Workers Launched', 'Single Copy', 'Plans'])\n",
      "INFO:root:idxs: [0]\n",
      "INFO:root:beginning js_node2dict(self, idx, node): returns a dictionary of 4 tensors per query plan\n",
      "INFO:root:returns a collated_dict of 4 tensors: 'x', 'attn_bias', 'rel_pos', 'heights \n",
      "INFO:root:nodeType: Gather\n",
      "INFO:root:typeId: 0\n",
      "INFO:root:formatFilter - filters: []\n",
      "INFO:root:formatFilter - alias: None\n",
      "INFO:root:formatJoin - join: None\n",
      "INFO:root:formatJoin - joinId: 0\n",
      "INFO:root:printing node features\n",
      "INFO:root:node.typeId: 0\n",
      "INFO:root:node.join: 0\n",
      "INFO:root:hists = filterDict2Hist(hist_file, node.filterDict, encoding): printing hists [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "INFO:root:len(hists) 150\n",
      "INFO:root:nodeType: Hash Join\n",
      "INFO:root:typeId: 1\n",
      "INFO:root:formatFilter - filters: []\n",
      "INFO:root:formatFilter - alias: None\n",
      "INFO:root:formatJoin - join: mi_idx.movie_id = t.id\n",
      "INFO:root:formatJoin - joinId: 1\n",
      "INFO:root:printing node features\n",
      "INFO:root:node.typeId: 1\n",
      "INFO:root:node.join: 1\n",
      "INFO:root:hists = filterDict2Hist(hist_file, node.filterDict, encoding): printing hists [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "INFO:root:len(hists) 150\n",
      "INFO:root:nodeType: Seq Scan\n",
      "INFO:root:typeId: 2\n",
      "INFO:root:formatFilter - filters: ['(kind_id = 7)']\n",
      "INFO:root:formatFilter - alias: t\n",
      "INFO:root:formatJoin - join: None\n",
      "INFO:root:formatJoin - joinId: 0\n",
      "INFO:root:printing node features\n",
      "INFO:root:node.typeId: 2\n",
      "INFO:root:node.join: 0\n",
      "INFO:root:hists = filterDict2Hist(hist_file, node.filterDict, encoding): printing hists [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "INFO:root:len(hists) 150\n",
      "INFO:root:nodeType: Hash\n",
      "INFO:root:typeId: 3\n",
      "INFO:root:formatFilter - filters: []\n",
      "INFO:root:formatFilter - alias: None\n",
      "INFO:root:formatJoin - join: None\n",
      "INFO:root:formatJoin - joinId: 0\n",
      "INFO:root:printing node features\n",
      "INFO:root:node.typeId: 3\n",
      "INFO:root:node.join: 0\n",
      "INFO:root:hists = filterDict2Hist(hist_file, node.filterDict, encoding): printing hists [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "INFO:root:len(hists) 150\n",
      "INFO:root:nodeType: Seq Scan\n",
      "INFO:root:typeId: 2\n",
      "INFO:root:formatFilter - filters: ['(info_type_id > 99)']\n",
      "INFO:root:formatFilter - alias: mi_idx\n",
      "INFO:root:formatJoin - join: None\n",
      "INFO:root:formatJoin - joinId: 0\n",
      "INFO:root:printing node features\n",
      "INFO:root:node.typeId: 2\n",
      "INFO:root:node.join: 0\n",
      "INFO:root:hists = filterDict2Hist(hist_file, node.filterDict, encoding): printing hists [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "INFO:root:len(hists) 150\n",
      "INFO:root:_dict's features: tensor([[ 0.,  0., 20.,  ...,  0.,  0.,  0.],\n",
      "        [ 1.,  1., 20.,  ...,  0.,  0.,  0.],\n",
      "        [ 2.,  0.,  1.,  ...,  0.,  0.,  0.],\n",
      "        [ 3.,  0., 20.,  ...,  0.,  0.,  0.],\n",
      "        [ 2.,  0., 16.,  ...,  0.,  1.,  1.]])\n",
      "INFO:root:_dict's features shape: torch.Size([5, 1165])\n",
      "INFO:root:_dict's heights: tensor([3, 2, 0, 1, 0])\n",
      "INFO:root:_dict's heights shape: torch.Size([5])\n",
      "INFO:root:_dict's adjacency_list: tensor([[0, 1],\n",
      "        [1, 2],\n",
      "        [1, 3],\n",
      "        [3, 4]])\n",
      "INFO:root:_dict's adjacency_list shape: torch.Size([4, 2])\n",
      "INFO:root:collated_dict's features shape: torch.Size([1, 30, 1165])\n",
      "INFO:root:collated_dict's attn_bias shape: torch.Size([1, 31, 31])\n",
      "INFO:root:collated_dict's rel_pos shape: torch.Size([1, 30, 30])\n",
      "INFO:root:collated_dict's heights shape: torch.Size([1, 30])\n",
      "INFO:root:collated_dicts: <class 'list'>\n",
      "INFO:root:collated_dicts (list) len: 1\n",
      "INFO:root:type(self.collated_dicts[0]): <class 'dict'>\n",
      "INFO:root:self.collated_dicts[0].keys(): dict_keys(['x', 'attn_bias', 'rel_pos', 'heights'])\n",
      "INFO:root:self.collated_dicts[0].keys()['x']: tensor([[[ 0.,  0., 20.,  ...,  0.,  0.,  0.],\n",
      "         [ 1.,  1., 20.,  ...,  0.,  0.,  0.],\n",
      "         [ 2.,  0.,  1.,  ...,  0.,  0.,  0.],\n",
      "         ...,\n",
      "         [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "         [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "         [ 1.,  1.,  1.,  ...,  1.,  1.,  1.]]])\n",
      "INFO:root:self.collated_dicts[0].keys()['x'].shape: torch.Size([1, 30, 1165])\n",
      "INFO:root:self.collated_dicts[0].keys()['attn_bias']: tensor([[[0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]]])\n",
      "INFO:root:self.collated_dicts[0].keys()['attn_bias'].shape: torch.Size([1, 31, 31])\n",
      "INFO:root:self.collated_dicts[0].keys()['rel_pos']: tensor([[[ 1,  2,  3,  3,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61,  1,  2,  2,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61,  1, 61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61,  1,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]])\n",
      "INFO:root:self.collated_dicts[0].keys()['rel_pos'].shape: torch.Size([1, 30, 30])\n",
      "INFO:root:self.collated_dicts[0].keys()['heights']: tensor([[4, 3, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]])\n",
      "INFO:root:self.collated_dicts[0].keys()['heights'].shape: torch.Size([1, 30])\n",
      "INFO:root:PlanTreeDataset initialized\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout)\n",
    "\n",
    "\n",
    "train_ds_single_query = PlanTreeDataset(train_df_single_query, None, encoding, hist_file, card_norm, cost_norm, to_predict, table_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5c1c6634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<model.dataset.PlanTreeDataset at 0x2c0afb500>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_single_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5181ae09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_ds_single_query[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4f7cc8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds_single_query[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c16ce308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_ds_single_query[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ab5ae021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['x', 'attn_bias', 'rel_pos', 'heights'])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_single_query[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "42d1c802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_ds_single_query[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ee356dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds_single_query[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "91856291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_ds_single_query[0][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "335e2f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6348, dtype=torch.float64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_single_query[0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f9f77c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_ds_single_query[0][1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dfe6b92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1167, dtype=torch.float64)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_single_query[0][1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3e976fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6348], dtype=torch.float64)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_single_query.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e52a6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "263ae8d7",
   "metadata": {},
   "source": [
    "# Exporling the Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "89d9d71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f4c8d577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model.model.QueryFormer"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c2ad3a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.loss.MSELoss"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(crit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cad2cfed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.Args"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6262ba9",
   "metadata": {},
   "source": [
    "# Making a forward pass through QueryFormer model using 1 Training Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0a2c9ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:QuerfyFormer forward\n",
      "INFO:root:x shape: torch.Size([1, 30, 1165])\n",
      "INFO:root:node_feature shape: torch.Size([1, 30, 329])\n",
      "INFO:root:super_token_feature shape: torch.Size([1, 1, 329])\n",
      "INFO:root:super_token_feature values: tensor([[[-3.6411e-01,  4.5445e-01, -5.2727e-01,  2.3610e-01,  1.9473e+00,\n",
      "          -1.2709e-01, -9.5878e-01, -1.6393e+00, -1.0220e+00, -6.3672e-01,\n",
      "          -5.4190e-03,  1.1480e+00,  8.1637e-01,  1.8798e+00,  1.4550e+00,\n",
      "          -8.9672e-01,  2.0786e+00,  4.2498e-01,  4.9432e-01,  1.2346e+00,\n",
      "          -5.9200e-02, -3.0412e-01,  4.5852e-01, -9.5504e-01, -4.8362e-01,\n",
      "           3.3751e-01, -8.2233e-01, -3.5166e-01, -1.9745e+00, -1.3450e+00,\n",
      "          -4.3359e-01,  1.7492e-02,  1.6619e+00,  2.8184e-01, -6.4980e-02,\n",
      "          -1.8063e+00, -1.5737e+00, -1.2378e+00, -7.8738e-01, -1.3226e+00,\n",
      "           1.0962e+00,  5.0930e-01, -2.2963e-01,  6.6542e-01,  7.6330e-01,\n",
      "          -7.5418e-01,  1.3894e+00,  4.3644e-01,  9.8246e-01, -2.6422e+00,\n",
      "          -1.1799e+00, -5.3285e-01, -7.8850e-02,  1.1959e+00,  1.2279e-01,\n",
      "           1.9685e+00,  6.3188e-02, -9.5557e-02,  2.3841e-01,  7.3634e-01,\n",
      "          -5.5243e-01, -4.2415e-01, -1.4709e+00,  1.7110e-01,  1.7306e-01,\n",
      "          -2.0630e+00, -1.7096e-02,  7.6806e-01, -1.3690e-01,  8.3649e-01,\n",
      "           1.2992e+00, -8.1223e-01,  7.9077e-01,  2.7423e-01, -4.8994e-01,\n",
      "           2.4836e+00,  1.3845e+00,  1.8486e-01, -1.1138e+00,  8.4550e-01,\n",
      "           1.3206e+00, -1.4299e+00,  5.1519e-01,  2.0189e-01,  3.8626e-02,\n",
      "           1.2533e+00,  9.8244e-02,  2.9911e-01, -1.2102e+00,  4.4186e-01,\n",
      "           1.1667e+00, -1.7390e+00,  9.1841e-01, -5.9943e-01, -4.7653e-01,\n",
      "          -3.0368e-01, -9.1109e-01, -1.2923e-01,  1.0285e+00,  1.2947e+00,\n",
      "          -7.8521e-01, -6.7668e-01, -1.4025e+00, -1.1639e+00, -4.1112e-01,\n",
      "          -9.0671e-02,  7.8295e-01, -1.6110e+00,  3.5024e-01, -8.1122e-01,\n",
      "           8.2557e-01,  3.0561e-01,  6.1131e-01,  1.8630e+00, -3.3098e-01,\n",
      "          -3.1290e-01, -1.6915e+00, -1.1088e+00, -6.1992e-01,  8.1413e-01,\n",
      "           5.0706e-01,  7.5620e-01,  2.8792e-01,  4.6520e-01,  1.1121e+00,\n",
      "          -1.7917e+00,  4.2311e-01,  9.2329e-01,  8.9289e-02,  6.6274e-01,\n",
      "           8.4119e-01, -1.8527e+00,  2.4893e+00,  6.8822e-01,  4.9753e-01,\n",
      "           1.7576e+00,  1.3873e+00,  1.8300e+00, -7.7375e-01,  5.0801e-01,\n",
      "           1.9118e+00,  2.0333e+00,  2.3667e-01, -4.9239e-01,  1.7563e+00,\n",
      "           8.1071e-01,  5.8867e-01, -1.6305e+00, -9.2656e-01,  4.3207e-01,\n",
      "           1.2722e-01,  1.4657e-01, -4.7223e-01, -4.1377e-02,  8.1026e-01,\n",
      "          -1.4187e+00, -2.1300e+00, -1.8799e-01, -4.8242e-01,  2.1575e+00,\n",
      "          -3.2844e+00, -2.0618e+00,  1.6475e+00, -5.3655e-01, -2.4509e+00,\n",
      "           2.7848e+00,  2.1492e-01,  5.5919e-01, -1.0617e+00, -6.1464e-01,\n",
      "          -1.0998e+00,  5.3985e-01, -1.4306e+00, -2.7807e+00,  6.3612e-01,\n",
      "           1.7976e+00,  4.2556e-01, -7.1420e-01,  4.8921e-01, -4.6866e-01,\n",
      "           5.7094e-01,  9.8889e-01, -4.2262e-01, -3.1509e-01, -1.6989e+00,\n",
      "           3.4523e-02, -2.7360e-01, -1.7190e+00, -7.2916e-01, -1.4958e+00,\n",
      "           3.5302e-01, -1.1449e-01,  6.1045e-01, -1.6658e+00, -1.6111e+00,\n",
      "          -1.8693e+00, -5.2054e-01, -1.7020e+00, -1.6486e-01,  7.9522e-01,\n",
      "          -2.2151e-01, -4.0250e-01, -1.5805e+00,  1.2218e-02,  1.8853e-01,\n",
      "           3.1141e-01,  2.5041e+00,  2.5804e-01,  9.5253e-02,  8.4190e-01,\n",
      "          -1.3792e+00, -2.8512e-01, -1.3685e-02, -9.0508e-03, -1.0399e+00,\n",
      "           1.2126e+00,  5.7824e-01, -4.6740e-02,  1.2254e+00,  1.0743e+00,\n",
      "           1.5469e+00, -4.6498e-01,  7.7728e-01,  2.7699e-01,  1.6938e-01,\n",
      "          -3.0947e-01, -9.7016e-01,  7.2639e-01, -5.7024e-01,  1.1762e+00,\n",
      "           1.5199e+00, -5.0850e-01, -1.2816e+00, -8.6552e-01,  1.9053e+00,\n",
      "          -2.2987e+00, -3.9453e-01, -2.7318e-01,  7.1949e-01, -1.3994e+00,\n",
      "          -3.9409e-01,  1.0011e+00,  1.4485e+00, -4.9984e-01, -5.2736e-01,\n",
      "          -1.1874e+00, -9.7223e-01, -2.1871e-01,  1.3283e+00,  5.3124e-01,\n",
      "           4.9436e-01, -1.8316e+00, -5.9387e-01, -5.2524e-01, -1.1188e+00,\n",
      "           3.3470e-01,  2.0593e+00,  1.6616e+00,  5.8967e-01, -7.3801e-01,\n",
      "          -6.7659e-01,  1.6688e+00, -1.7168e-02,  9.0070e-01, -8.8435e-01,\n",
      "          -2.6802e-01, -3.5494e-01,  9.4192e-02,  2.1674e-01, -1.3290e+00,\n",
      "           5.4358e-01,  2.1613e-01, -4.4984e-01, -3.7402e-01,  2.1335e+00,\n",
      "           2.7120e-01,  1.1868e-01,  9.2353e-01,  6.3601e-01,  2.6176e-01,\n",
      "          -6.1332e-01, -3.8789e-01, -1.5086e-01,  3.3167e-01, -1.5152e+00,\n",
      "           9.9139e-01,  6.5970e-01,  9.6085e-01,  1.9821e-01, -4.8720e-01,\n",
      "           9.4253e-01, -4.9734e-02,  5.9831e-01,  1.7842e-01,  3.0642e-01,\n",
      "           1.4232e-01, -1.3184e+00, -4.9745e-01,  4.8416e-01,  1.2601e+00,\n",
      "          -1.5495e+00, -3.5913e-01, -1.0940e+00, -1.4415e-01,  1.6003e+00,\n",
      "           2.4122e-03,  8.5193e-01, -2.9664e+00, -2.8483e-01, -7.9670e-01,\n",
      "           2.9618e-01, -1.7781e-01, -8.3223e-01,  1.7884e+00,  1.2451e+00,\n",
      "          -2.2364e-02,  1.3234e+00,  2.1098e+00,  1.4594e-02,  1.1340e+00,\n",
      "           1.3832e+00, -3.9092e-01, -1.7486e+00, -1.2202e+00, -1.2292e+00,\n",
      "           3.4105e-01, -1.8087e-01, -2.1934e-02,  1.1370e-01]]],\n",
      "       grad_fn=<RepeatBackward0>)\n",
      "INFO:root:super_node_feature shape: torch.Size([1, 31, 329])\n",
      "INFO:root:output shape: torch.Size([1, 31, 329])\n",
      "INFO:root:Super node feature dimensions: torch.Size([1, 31, 329])\n",
      "INFO:root:output[:,0,:].shape: torch.Size([1, 329])\n",
      "INFO:root:output[:,0,:]: tensor([[ 3.5161e-01,  6.0907e-01, -7.1125e-01, -3.8866e-01,  6.5022e-01,\n",
      "          3.9880e-02, -1.1369e+00, -3.6793e-01,  4.5950e-01, -5.0162e-02,\n",
      "         -4.5810e-01,  6.8180e-01,  8.9130e-01, -3.8042e-01,  1.2142e+00,\n",
      "         -5.6733e-01,  1.1843e+00,  1.0314e+00,  4.7517e-01,  1.3743e+00,\n",
      "         -1.0438e+00,  4.8798e-01,  8.6579e-01, -7.0713e-01,  1.5092e-02,\n",
      "          5.5431e-01, -1.0384e+00,  1.3108e-01, -1.9516e+00, -1.5822e+00,\n",
      "         -2.5402e-02,  8.0929e-02,  1.6919e+00, -8.0040e-01,  6.5333e-01,\n",
      "         -2.5005e+00, -3.1604e-01, -5.3521e-01,  1.6532e-01, -1.4766e+00,\n",
      "          1.1120e+00,  6.9375e-01, -6.9632e-02,  2.3387e+00,  5.4812e-01,\n",
      "         -9.7732e-01,  1.2742e+00,  4.1775e-01,  7.7811e-01, -2.5736e+00,\n",
      "         -5.0270e-01, -1.0802e+00,  7.8091e-01,  2.1313e+00, -1.7803e+00,\n",
      "          1.5244e+00, -5.1840e-01,  5.0030e-01, -5.5668e-01,  6.6282e-01,\n",
      "         -2.9377e-01,  2.6897e-01, -1.2095e+00, -2.3689e-01,  7.3913e-01,\n",
      "         -9.4062e-01, -7.9983e-03,  6.4814e-01, -9.0484e-01,  1.0986e+00,\n",
      "          4.6634e-02, -7.6134e-01,  1.3568e-01,  1.9226e-01, -1.1036e-01,\n",
      "          1.2064e+00, -1.0331e+00,  1.3026e-03, -8.4795e-01,  9.9265e-01,\n",
      "          1.4589e+00, -1.7158e+00,  7.6180e-02,  1.8293e-01,  1.0114e-01,\n",
      "          6.8001e-01, -3.7109e-01,  6.0686e-01, -1.2372e+00, -7.3702e-01,\n",
      "          3.1754e-01, -5.2147e-01,  2.5690e-01, -5.3337e-01,  6.4959e-01,\n",
      "         -6.5575e-01, -4.6559e-01,  3.8646e-01,  4.7342e-01,  6.6850e-01,\n",
      "         -6.4299e-01, -1.2534e+00, -1.6407e+00, -1.2436e+00,  2.0771e-01,\n",
      "          8.3893e-01,  4.7976e-01, -1.1601e+00,  9.0470e-01, -4.4001e-01,\n",
      "          1.3484e+00, -1.4422e-01,  3.5345e-01, -7.3806e-01,  3.6036e-01,\n",
      "         -5.5067e-01, -1.2571e+00, -8.1352e-01, -1.1391e+00,  6.7507e-01,\n",
      "          1.2264e+00,  8.5093e-01,  4.3801e-01,  2.7060e-01, -3.2478e-02,\n",
      "         -9.7670e-01, -2.3099e-01,  9.7183e-01,  4.1349e-01,  6.1744e-01,\n",
      "          2.0730e-01, -7.7731e-01,  2.2690e+00,  4.0208e-01,  4.5612e-01,\n",
      "          1.4217e+00,  1.0130e+00,  2.1115e+00, -1.1899e+00, -7.7028e-01,\n",
      "          1.7325e+00,  4.1012e-01, -9.7470e-02, -8.5347e-01,  1.0554e+00,\n",
      "          7.3719e-02,  3.8241e-01, -1.6859e+00, -7.8143e-01,  9.7843e-01,\n",
      "         -1.7958e-01, -3.8343e-01, -6.1057e-01,  3.6388e-01,  6.9309e-01,\n",
      "         -4.1870e-02, -1.3029e+00,  8.1522e-01, -5.6746e-01,  2.5015e+00,\n",
      "         -2.5701e+00, -1.9815e+00,  1.3411e+00, -1.3718e+00, -1.6550e+00,\n",
      "          2.5567e+00,  2.2939e-01,  1.6672e-01, -1.1819e+00, -2.5109e-01,\n",
      "         -5.3315e-01, -1.3411e-01, -4.2880e-01, -3.1685e+00,  1.5285e+00,\n",
      "          1.4678e+00, -3.0595e-02, -6.6293e-01,  1.4695e-01,  1.1394e+00,\n",
      "          5.5965e-01,  1.0567e+00,  1.7252e-01, -1.4807e+00, -1.1236e+00,\n",
      "          1.3121e-01,  2.8279e-01, -2.1644e+00, -2.9414e-02, -1.7143e+00,\n",
      "         -7.3678e-01, -1.4742e-01,  3.4535e-01, -1.3871e+00, -1.2775e+00,\n",
      "         -1.6518e+00, -5.3488e-01, -1.8094e+00,  4.6542e-02,  3.2904e-01,\n",
      "          2.1374e-01, -2.0330e-01, -5.4262e-01, -5.7793e-01, -1.1681e+00,\n",
      "          5.7422e-01,  1.3268e+00,  6.1640e-01, -4.5205e-01, -1.6621e-01,\n",
      "         -7.7296e-01,  1.1189e+00,  2.0181e-01, -3.4671e-01,  8.7219e-02,\n",
      "          1.7514e+00,  4.7856e-01,  8.4356e-01,  3.1571e-01,  1.0860e+00,\n",
      "          2.5993e+00,  4.7379e-01,  1.0674e+00, -1.7954e-01,  7.2944e-01,\n",
      "         -8.8313e-01, -1.1137e+00, -5.5801e-01, -1.6688e-01,  1.9439e+00,\n",
      "          1.7170e+00, -8.2442e-01, -1.7380e+00,  3.0024e-02,  1.7274e+00,\n",
      "         -2.7976e+00,  4.3490e-01,  8.2709e-01,  1.1421e+00, -6.2546e-01,\n",
      "         -1.5723e-01,  8.3674e-01,  1.9559e+00, -4.4987e-01,  2.4147e-01,\n",
      "         -1.3357e+00, -6.8914e-01, -5.3542e-01,  7.8743e-01,  4.9087e-01,\n",
      "          1.2845e+00, -7.5673e-01, -6.4868e-01, -2.7659e-01, -9.2088e-01,\n",
      "          8.1681e-01,  5.1059e-01,  1.5161e+00, -8.8196e-02, -1.3792e-02,\n",
      "         -6.7576e-01,  9.1660e-01,  3.6946e-01,  1.1914e+00, -4.2596e-01,\n",
      "         -4.2374e-01, -3.1025e-01, -2.7838e-01,  2.5919e-03, -1.9579e+00,\n",
      "          6.1677e-01,  5.8483e-01, -1.4484e-01,  3.7224e-01, -3.7297e-01,\n",
      "         -1.1701e-01, -2.3532e-01,  2.4452e-01, -8.0214e-01, -6.2306e-01,\n",
      "         -1.6867e-01,  7.9211e-01,  1.2820e+00,  9.5589e-01, -1.1161e+00,\n",
      "          1.6404e+00, -1.4665e-01,  9.1100e-02,  1.0509e+00, -5.5291e-01,\n",
      "         -3.8083e-01,  1.7753e-01,  1.2255e+00, -4.8888e-01,  1.2021e-01,\n",
      "         -5.4866e-01, -1.1250e+00, -2.7997e-01,  7.9278e-01,  6.6955e-01,\n",
      "         -9.2526e-01,  3.4372e-01, -6.6216e-01,  2.8552e-01,  1.4014e+00,\n",
      "          3.1610e-01,  3.3369e-01, -3.6652e+00, -3.9955e-01, -3.7784e-01,\n",
      "         -8.9767e-01, -5.0170e-01, -1.1894e-01,  1.1907e+00,  1.3298e+00,\n",
      "          7.7412e-01,  1.3967e+00, -6.8778e-01,  2.6894e-01,  9.9198e-01,\n",
      "          1.6398e+00, -1.4883e+00, -7.8684e-01, -1.1506e+00, -1.4453e+00,\n",
      "          3.6571e-01, -8.8131e-01,  6.9134e-01,  2.8832e-01]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shaikhq/coding/QueryFormer/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "crit = nn.MSELoss()\n",
    "model = train_single(model, train_ds_single_query, train_ds_single_query, crit, cost_norm, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
