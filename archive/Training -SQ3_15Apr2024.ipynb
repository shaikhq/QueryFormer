{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9c5213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "037bec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.util import Normalizer\n",
    "from model.database_util import get_hist_file, get_job_table_sample, collator\n",
    "from model.model import QueryFormer\n",
    "from model.database_util import Encoding\n",
    "from model.dataset import PlanTreeDataset\n",
    "from model.trainer import eval_workload, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "822fdcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/imdb/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fbcd4773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./results/full/cost/\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    bs = 1024\n",
    "    lr = 0.001\n",
    "    # SQ: changed the epochs to 1 to run faster\n",
    "    # epochs = 200\n",
    "    epochs = 1\n",
    "    clip_size = 50\n",
    "    embed_size = 64 \n",
    "    pred_hid = 128\n",
    "    ffn_dim = 128\n",
    "    head_size = 12\n",
    "    n_layers = 8\n",
    "    dropout = 0.1\n",
    "    sch_decay = 0.6\n",
    "    # SQ: changed to cpu\n",
    "    # device = 'cuda:0'\n",
    "    device = 'cpu'\n",
    "    newpath = './results/full/cost/'\n",
    "    to_predict = 'cost'\n",
    "args = Args()\n",
    "\n",
    "import os\n",
    "if not os.path.exists(args.newpath):\n",
    "    os.makedirs(args.newpath)\n",
    "print(args.newpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aace65f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/db2inst1/sq-wlm/QueryFormer/model/database_util.py:108: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  hist_file['bins'][rid] = \\\n",
      "/home/db2inst1/sq-wlm/QueryFormer/model/database_util.py:108: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  hist_file['bins'][rid] = \\\n",
      "/home/db2inst1/sq-wlm/QueryFormer/model/database_util.py:108: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  hist_file['bins'][rid] = \\\n",
      "/home/db2inst1/sq-wlm/QueryFormer/model/database_util.py:108: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  hist_file['bins'][rid] = \\\n",
      "/home/db2inst1/sq-wlm/QueryFormer/model/database_util.py:108: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  hist_file['bins'][rid] = \\\n",
      "/home/db2inst1/sq-wlm/QueryFormer/model/database_util.py:108: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  hist_file['bins'][rid] = \\\n",
      "/home/db2inst1/sq-wlm/QueryFormer/model/database_util.py:108: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  hist_file['bins'][rid] = \\\n",
      "/home/db2inst1/sq-wlm/QueryFormer/model/database_util.py:108: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  hist_file['bins'][rid] = \\\n",
      "/home/db2inst1/sq-wlm/QueryFormer/model/database_util.py:108: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  hist_file['bins'][rid] = \\\n"
     ]
    }
   ],
   "source": [
    "# What is in this histogram file?\n",
    "hist_file = get_hist_file(data_path + 'histogram_string.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efc31e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c639e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table</th>\n",
       "      <th>column</th>\n",
       "      <th>freq</th>\n",
       "      <th>bins</th>\n",
       "      <th>table_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>title</td>\n",
       "      <td>production_year</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[1880, 1913, 1923, 1942, 1955, 1960, 1964, 196...</td>\n",
       "      <td>t.production_year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>title</td>\n",
       "      <td>kind_id</td>\n",
       "      <td>[0.0, 0.26216118191156074, 0.03593387047716835...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, ...</td>\n",
       "      <td>t.kind_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>movie_companies</td>\n",
       "      <td>company_id</td>\n",
       "      <td>[0.0, 0.0004959511376981121, 0.000558807386989...</td>\n",
       "      <td>[1, 6, 19, 27, 68, 133, 160, 189, 292, 402, 47...</td>\n",
       "      <td>mc.company_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>movie_companies</td>\n",
       "      <td>company_type_id</td>\n",
       "      <td>[0.0, 0.4883796425472418, 0.5116203574527581]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>mc.company_type_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cast_info</td>\n",
       "      <td>role_id</td>\n",
       "      <td>[0.0, 0.3495907485479872, 0.20560375449487386,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>ci.role_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>movie_keyword</td>\n",
       "      <td>keyword_id</td>\n",
       "      <td>[0.0, 0.0031748950967179193, 1.989421142551088...</td>\n",
       "      <td>[1, 77, 132, 230, 331, 347, 384, 495, 643, 784...</td>\n",
       "      <td>mk.keyword_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cast_info</td>\n",
       "      <td>person_id</td>\n",
       "      <td>[0.0, 0.0, 5.518102507748589e-08, 2.7590512538...</td>\n",
       "      <td>[2, 77446, 145798, 212750, 281691, 347240, 419...</td>\n",
       "      <td>ci.person_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>movie_info_idx</td>\n",
       "      <td>info_type_id</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 9...</td>\n",
       "      <td>mi_idx.info_type_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>movie_info</td>\n",
       "      <td>info_type_id</td>\n",
       "      <td>[0.0, 0.05406815807174563, 0.08688004942665738...</td>\n",
       "      <td>[1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, ...</td>\n",
       "      <td>mi.info_type_id</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             table           column  \\\n",
       "0            title  production_year   \n",
       "1            title          kind_id   \n",
       "2  movie_companies       company_id   \n",
       "3  movie_companies  company_type_id   \n",
       "4        cast_info          role_id   \n",
       "5    movie_keyword       keyword_id   \n",
       "6        cast_info        person_id   \n",
       "7   movie_info_idx     info_type_id   \n",
       "8       movie_info     info_type_id   \n",
       "\n",
       "                                                freq  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.26216118191156074, 0.03593387047716835...   \n",
       "2  [0.0, 0.0004959511376981121, 0.000558807386989...   \n",
       "3      [0.0, 0.4883796425472418, 0.5116203574527581]   \n",
       "4  [0.0, 0.3495907485479872, 0.20560375449487386,...   \n",
       "5  [0.0, 0.0031748950967179193, 1.989421142551088...   \n",
       "6  [0.0, 0.0, 5.518102507748589e-08, 2.7590512538...   \n",
       "7  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "8  [0.0, 0.05406815807174563, 0.08688004942665738...   \n",
       "\n",
       "                                                bins         table_column  \n",
       "0  [1880, 1913, 1923, 1942, 1955, 1960, 1964, 196...    t.production_year  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, ...            t.kind_id  \n",
       "2  [1, 6, 19, 27, 68, 133, 160, 189, 292, 402, 47...        mc.company_id  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   mc.company_type_id  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...           ci.role_id  \n",
       "5  [1, 77, 132, 230, 331, 347, 384, 495, 643, 784...        mk.keyword_id  \n",
       "6  [2, 77446, 145798, 212750, 281691, 347240, 419...         ci.person_id  \n",
       "7  [99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 9...  mi_idx.info_type_id  \n",
       "8  [1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, ...      mi.info_type_id  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_file.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5552116",
   "metadata": {},
   "source": [
    "### Questions on hist_file:\n",
    "1. What does this file contain?\n",
    "2. Why are there two columns with the same value: i.e., column and table_column?\n",
    "3. What are frequencies?\n",
    "4. What are bins?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ab09049",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_norm = Normalizer(-3.61192, 12.290855)\n",
    "card_norm = Normalizer(1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57b0b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_ckpt = torch.load('checkpoints/encoding.pt')\n",
    "# SQ: what's in the encoding.pt file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e992eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['encoding'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_ckpt.keys()\n",
    "# Whose encoding?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3116f5d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoding': <model.database_util.Encoding at 0x7fd67c84a8d0>}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea8aaa02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding\n",
      "<model.database_util.Encoding object at 0x7fd67c84a8d0>\n"
     ]
    }
   ],
   "source": [
    "for key, value in encoding_ckpt.items():\n",
    "    print(key)\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dad256a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col-min-max: {'t.id': [1.0, 2528312.0], 't.kind_id': [1.0, 7.0], 't.production_year': [1880.0, 2019.0], 'mc.id': [1.0, 2609129.0], 'mc.company_id': [1.0, 234997.0], 'mc.movie_id': [2.0, 2525745.0], 'mc.company_type_id': [1.0, 2.0], 'ci.id': [1.0, 36244344.0], 'ci.movie_id': [1.0, 2525975.0], 'ci.person_id': [1.0, 4061926.0], 'ci.role_id': [1.0, 11.0], 'mi.id': [1.0, 14835720.0], 'mi.movie_id': [1.0, 2526430.0], 'mi.info_type_id': [1.0, 110.0], 'mi_idx.id': [1.0, 1380035.0], 'mi_idx.movie_id': [2.0, 2525793.0], 'mi_idx.info_type_id': [99.0, 113.0], 'mk.id': [1.0, 4523930.0], 'mk.movie_id': [2.0, 2525971.0], 'mk.keyword_id': [1.0, 134170.0]}\n",
      "---------\n",
      "col-index: {'t.id': 0, 't.kind_id': 1, 't.production_year': 2, 'mc.id': 3, 'mc.company_id': 4, 'mc.movie_id': 5, 'mc.company_type_id': 6, 'ci.id': 7, 'ci.movie_id': 8, 'ci.person_id': 9, 'ci.role_id': 10, 'mi.id': 11, 'mi.movie_id': 12, 'mi.info_type_id': 13, 'mi_idx.id': 14, 'mi_idx.movie_id': 15, 'mi_idx.info_type_id': 16, 'mk.id': 17, 'mk.movie_id': 18, 'mk.keyword_id': 19, 'NA': 20}\n",
      "---------\n",
      "op-index: {'>': 0, '=': 1, '<': 2, 'NA': 3}\n",
      "---------\n",
      "type-index: {'Gather': 0, 'Hash Join': 1, 'Seq Scan': 2, 'Hash': 3, 'Bitmap Heap Scan': 4, 'Bitmap Index Scan': 5, 'Nested Loop': 6, 'Index Scan': 7, 'Merge Join': 8, 'Gather Merge': 9, 'Materialize': 10, 'BitmapAnd': 11, 'Sort': 12}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Iterate over the key-value pairs\n",
    "for key, value in encoding_ckpt.items():\n",
    "    \n",
    "    print(f\"col-min-max: {value.column_min_max_vals}\")\n",
    "    print(\"---------\")\n",
    "    print(f\"col-index: {value.col2idx}\")\n",
    "    print(\"---------\")\n",
    "    print(f\"op-index: {value.op2idx}\")\n",
    "    print(\"---------\")\n",
    "    print(f\"type-index: {value.type2idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8ea5be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = encoding_ckpt['encoding']\n",
    "# SQ: what is in encoding? \n",
    "# SQ: how was this encoding created?\n",
    "# SQ: Is this encoding specific to a workload? or a dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a30d6e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model.database_util.Encoding"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fc45567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQ: from looking into the definition of Encoding class, it appears that its a data structure for \n",
    "# storing the encoding information of a dataset? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93decbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('checkpoints/cost_model.pt', map_location='cpu')\n",
    "# SQ: what is the cost model?\n",
    "# SQ: what is being loaded here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aac8ad09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "798f6c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['model', 'args'])\n",
      "1024\n",
      "0.001\n",
      "1\n",
      "50\n",
      "64\n",
      "128\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "# checkpoint.keys()\n",
    "print(checkpoint.keys())\n",
    "\n",
    "\n",
    "\n",
    "print(checkpoint[\"args\"].bs)\n",
    "print(checkpoint[\"args\"].lr)\n",
    "print(checkpoint[\"args\"].epochs)\n",
    "print(checkpoint[\"args\"].clip_size)\n",
    "print(checkpoint[\"args\"].embed_size)\n",
    "print(checkpoint[\"args\"].pred_hid)\n",
    "print(checkpoint[\"args\"].ffn_dim)\n",
    "\n",
    "# YA: They store the model and its hyperparameter in Args class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8c66970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQ: Is this model already trained? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71759b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.util import seed_everything\n",
    "seed_everything()\n",
    "# This function seed_everything() is used to set a fixed seed for random number generators in Python, NumPy, and PyTorch. \n",
    "# This is done to ensure that the results of your code are reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9592f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QueryFormer(emb_size = args.embed_size ,ffn_dim = args.ffn_dim, head_size = args.head_size, \\\n",
    "                 dropout = args.dropout, n_layers = args.n_layers, \\\n",
    "                 use_sample = True, use_hist = True, \\\n",
    "                 pred_hid = args.pred_hid\n",
    "                )\n",
    "# emb_size = length of each embedding vector. Will we have 1 embedding vector per node in the query plan tree?\n",
    "# embeddings are used to convert categorical variables into continuous ones that can be processed by the model.\n",
    "# ffn_dim = the number of nodes in each hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9334f43c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model.model.QueryFormer"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd089472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QueryFormer(\n",
      "  (rel_pos_encoder): Embedding(64, 12, padding_idx=0)\n",
      "  (height_encoder): Embedding(64, 329, padding_idx=0)\n",
      "  (input_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (layers): ModuleList(\n",
      "    (0-7): 8 x EncoderLayer(\n",
      "      (self_attention_norm): LayerNorm((329,), eps=1e-05, elementwise_affine=True)\n",
      "      (self_attention): MultiHeadAttention(\n",
      "        (linear_q): Linear(in_features=329, out_features=324, bias=True)\n",
      "        (linear_k): Linear(in_features=329, out_features=324, bias=True)\n",
      "        (linear_v): Linear(in_features=329, out_features=324, bias=True)\n",
      "        (att_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (output_layer): Linear(in_features=324, out_features=329, bias=True)\n",
      "      )\n",
      "      (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (ffn_norm): LayerNorm((329,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): FeedForwardNetwork(\n",
      "        (layer1): Linear(in_features=329, out_features=128, bias=True)\n",
      "        (gelu): GELU(approximate='none')\n",
      "        (layer2): Linear(in_features=128, out_features=329, bias=True)\n",
      "      )\n",
      "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_ln): LayerNorm((329,), eps=1e-05, elementwise_affine=True)\n",
      "  (super_token): Embedding(1, 329)\n",
      "  (super_token_virtual_distance): Embedding(1, 12)\n",
      "  (embbed_layer): FeatureEmbed(\n",
      "    (typeEmbed): Embedding(20, 64)\n",
      "    (tableEmbed): Embedding(10, 64)\n",
      "    (columnEmbed): Embedding(30, 64)\n",
      "    (opEmbed): Embedding(4, 8)\n",
      "    (linearFilter2): Linear(in_features=73, out_features=73, bias=True)\n",
      "    (linearFilter): Linear(in_features=73, out_features=73, bias=True)\n",
      "    (linearType): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (linearJoin): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (linearSample): Linear(in_features=1000, out_features=64, bias=True)\n",
      "    (linearHist): Linear(in_features=50, out_features=64, bias=True)\n",
      "    (joinEmbed): Embedding(40, 64)\n",
      "    (project): Linear(in_features=329, out_features=329, bias=True)\n",
      "  )\n",
      "  (pred): Prediction(\n",
      "    (out_mlp1): Linear(in_features=329, out_features=128, bias=True)\n",
      "    (mid_mlp1): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (mid_mlp2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (out_mlp2): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      "  (pred2): Prediction(\n",
      "    (out_mlp1): Linear(in_features=329, out_features=128, bias=True)\n",
      "    (mid_mlp1): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (mid_mlp2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (out_mlp2): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85e27584",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c8a7d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict = 'cost'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a10145b",
   "metadata": {},
   "source": [
    "## SQ: Loading training set\n",
    "The following code block is loading the training set from a set of files and concatenating them to produce a final dataframe training set\n",
    "1. What's the structure of the training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4286ce8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       {\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa...\n",
      "1       {\"Plan\": {\"Node Type\": \"Seq Scan\", \"Parallel A...\n",
      "2       {\"Plan\": {\"Node Type\": \"Seq Scan\", \"Parallel A...\n",
      "3       {\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa...\n",
      "4       {\"Plan\": {\"Node Type\": \"Bitmap Heap Scan\", \"Pa...\n",
      "                              ...                        \n",
      "4995    {\"Plan\": {\"Node Type\": \"Hash Join\", \"Parallel ...\n",
      "4996    {\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa...\n",
      "4997    {\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa...\n",
      "4998    {\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa...\n",
      "4999    {\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa...\n",
      "Name: json, Length: 10000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "imdb_path = './data/imdb/'\n",
    "full_train_df = pd.DataFrame()\n",
    "#for i in range(18):\n",
    "for i in range(2):\n",
    "    file = imdb_path + 'plan_and_cost/train_plan_part{}.csv'.format(i)\n",
    "    df = pd.read_csv(file)\n",
    "    # print(df.shape)\n",
    "    ## SQ: the following line of coding is giving this error: AttributeError: 'DataFrame' object has no attribute 'append'\n",
    "    # full_train_df = full_train_df.append(df)\n",
    "    # SQ: changing the code\n",
    "    full_train_df = pd.concat([full_train_df, df])\n",
    "\n",
    "print(full_train_df['json'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75d7e32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded queries with len  100000\n",
      "Loaded bitmaps\n"
     ]
    }
   ],
   "source": [
    "val_df = pd.DataFrame()\n",
    "for i in range(18,20):\n",
    "    file = imdb_path + 'plan_and_cost/train_plan_part{}.csv'.format(i)\n",
    "    df = pd.read_csv(file)\n",
    "    ## SQ: the following line of coding is giving this error: AttributeError: 'DataFrame' object has no attribute 'append'\n",
    "    # val_df = val_df.append(df)\n",
    "    # SQ: changed the following to the following\n",
    "    val_df = pd.concat([val_df, df])\n",
    "table_sample = get_job_table_sample(imdb_path+'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fca0f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Aware\": false, \"Startup Cost\": 23540.58, \"Total Cost\": 154548.95, \"Plan Rows\": 567655, \"Plan Width\": 119, \"Actual Startup Time\": 386.847, \"Actual Total Time\": 646.972, \"Actual Rows\": 283812, \"Actual Loops\": 1, \"Workers Planned\": 2, \"Workers Launched\": 2, \"Single Copy\": false, \"Plans\": [{\"Node Type\": \"Hash Join\", \"Parent Relationship\": \"Outer\", \"Parallel Aware\": true, \"Join Type\": \"Inner\", \"Startup Cost\": 22540.58, \"Total Cost\": 96783.45, \"Plan Rows\": 236523, \"Plan Width\": 119, \"Actual Startup Time\": 369.985, \"Actual Total Time\": 518.487, \"Actual Rows\": 94604, \"Actual Loops\": 3, \"Inner Unique\": false, \"Hash Cond\": \"(t.id = mi_idx.movie_id)\", \"Workers\": [], \"Plans\": [{\"Node Type\": \"Seq Scan\", \"Parent Relationship\": \"Outer\", \"Parallel Aware\": true, \"Relation Name\": \"title\", \"Alias\": \"t\", \"Startup Cost\": 0.0, \"Total Cost\": 49166.46, \"Plan Rows\": 649574, \"Plan Width\": 94, \"Actual Startup Time\": 0.366, \"Actual Total Time\": 147.047, \"Actual Rows\": 514421, \"Actual Loops\": 3, \"Filter\": \"(kind_id = 7)\", \"Rows Removed by Filter\": 328349, \"Workers\": []}, {\"Node Type\": \"Hash\", \"Parent Relationship\": \"Inner\", \"Parallel Aware\": true, \"Startup Cost\": 15122.68, \"Total Cost\": 15122.68, \"Plan Rows\": 383592, \"Plan Width\": 25, \"Actual Startup Time\": 103.547, \"Actual Total Time\": 103.547, \"Actual Rows\": 306703, \"Actual Loops\": 3, \"Hash Buckets\": 65536, \"Original Hash Buckets\": 65536, \"Hash Batches\": 32, \"Original Hash Batches\": 32, \"Peak Memory Usage\": 1920, \"Workers\": [], \"Plans\": [{\"Node Type\": \"Seq Scan\", \"Parent Relationship\": \"Outer\", \"Parallel Aware\": true, \"Relation Name\": \"movie_info_idx\", \"Alias\": \"mi_idx\", \"Startup Cost\": 0.0, \"Total Cost\": 15122.68, \"Plan Rows\": 383592, \"Plan Width\": 25, \"Actual Startup Time\": 0.28, \"Actual Total Time\": 54.382, \"Actual Rows\": 306703, \"Actual Loops\": 3, \"Filter\": \"(info_type_id > 99)\", \"Rows Removed by Filter\": 153308, \"Workers\": []}]}]}]}, \"Planning Time\": 2.382, \"Triggers\": [], \"Execution Time\": 654.241}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_322315/1381683576.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  specific_value = full_train_df.iloc[0][1]\n"
     ]
    }
   ],
   "source": [
    "# YA: Reading the first query plan\n",
    "specific_value = full_train_df.iloc[0][1]\n",
    "print(specific_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "815c12d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a66ad18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>{\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>{\"Plan\": {\"Node Type\": \"Seq Scan\", \"Parallel A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>{\"Plan\": {\"Node Type\": \"Seq Scan\", \"Parallel A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>{\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>{\"Plan\": {\"Node Type\": \"Bitmap Heap Scan\", \"Pa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               json\n",
       "0   0  {\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa...\n",
       "1   1  {\"Plan\": {\"Node Type\": \"Seq Scan\", \"Parallel A...\n",
       "2   2  {\"Plan\": {\"Node Type\": \"Seq Scan\", \"Parallel A...\n",
       "3   3  {\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa...\n",
       "4   4  {\"Plan\": {\"Node Type\": \"Bitmap Heap Scan\", \"Pa..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8378beb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id       int64\n",
       "json    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18ac4927",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = full_train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2396baa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>{\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>{\"Plan\": {\"Node Type\": \"Seq Scan\", \"Parallel A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>{\"Plan\": {\"Node Type\": \"Seq Scan\", \"Parallel A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>{\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>{\"Plan\": {\"Node Type\": \"Bitmap Heap Scan\", \"Pa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               json\n",
       "0   0  {\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa...\n",
       "1   1  {\"Plan\": {\"Node Type\": \"Seq Scan\", \"Parallel A...\n",
       "2   2  {\"Plan\": {\"Node Type\": \"Seq Scan\", \"Parallel A...\n",
       "3   3  {\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa...\n",
       "4   4  {\"Plan\": {\"Node Type\": \"Bitmap Heap Scan\", \"Pa..."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a375a1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f8af3bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Node Type': 'Gather', 'Parallel Aware': False, 'Startup Cost': 23540.58, 'Total Cost': 154548.95, 'Plan Rows': 567655, 'Plan Width': 119, 'Actual Startup Time': 386.847, 'Actual Total Time': 646.972, 'Actual Rows': 283812, 'Actual Loops': 1, 'Workers Planned': 2, 'Workers Launched': 2, 'Single Copy': False, 'Plans': [{'Node Type': 'Hash Join', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Join Type': 'Inner', 'Startup Cost': 22540.58, 'Total Cost': 96783.45, 'Plan Rows': 236523, 'Plan Width': 119, 'Actual Startup Time': 369.985, 'Actual Total Time': 518.487, 'Actual Rows': 94604, 'Actual Loops': 3, 'Inner Unique': False, 'Hash Cond': '(t.id = mi_idx.movie_id)', 'Workers': [], 'Plans': [{'Node Type': 'Seq Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Relation Name': 'title', 'Alias': 't', 'Startup Cost': 0.0, 'Total Cost': 49166.46, 'Plan Rows': 649574, 'Plan Width': 94, 'Actual Startup Time': 0.366, 'Actual Total Time': 147.047, 'Actual Rows': 514421, 'Actual Loops': 3, 'Filter': '(kind_id = 7)', 'Rows Removed by Filter': 328349, 'Workers': []}, {'Node Type': 'Hash', 'Parent Relationship': 'Inner', 'Parallel Aware': True, 'Startup Cost': 15122.68, 'Total Cost': 15122.68, 'Plan Rows': 383592, 'Plan Width': 25, 'Actual Startup Time': 103.547, 'Actual Total Time': 103.547, 'Actual Rows': 306703, 'Actual Loops': 3, 'Hash Buckets': 65536, 'Original Hash Buckets': 65536, 'Hash Batches': 32, 'Original Hash Batches': 32, 'Peak Memory Usage': 1920, 'Workers': [], 'Plans': [{'Node Type': 'Seq Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Relation Name': 'movie_info_idx', 'Alias': 'mi_idx', 'Startup Cost': 0.0, 'Total Cost': 15122.68, 'Plan Rows': 383592, 'Plan Width': 25, 'Actual Startup Time': 0.28, 'Actual Total Time': 54.382, 'Actual Rows': 306703, 'Actual Loops': 3, 'Filter': '(info_type_id > 99)', 'Rows Removed by Filter': 153308, 'Workers': []}]}]}]}, {'Node Type': 'Seq Scan', 'Parallel Aware': False, 'Relation Name': 'title', 'Alias': 't', 'Startup Cost': 0.0, 'Total Cost': 67602.3, 'Plan Rows': 1116092, 'Plan Width': 94, 'Actual Startup Time': 0.035, 'Actual Total Time': 322.837, 'Actual Rows': 1107925, 'Actual Loops': 1, 'Filter': '(production_year > 2004)', 'Rows Removed by Filter': 1420387}, {'Node Type': 'Seq Scan', 'Parallel Aware': False, 'Relation Name': 'movie_info', 'Alias': 'mi', 'Startup Cost': 0.0, 'Total Cost': 347461.15, 'Plan Rows': 3700158, 'Plan Width': 74, 'Actual Startup Time': 188.05, 'Actual Total Time': 1611.147, 'Actual Rows': 3624977, 'Actual Loops': 1, 'Filter': '(info_type_id < 4)', 'Rows Removed by Filter': 11210743}, {'Node Type': 'Gather', 'Parallel Aware': False, 'Startup Cost': 22727.12, 'Total Cost': 87837.1, 'Plan Rows': 130827, 'Plan Width': 134, 'Actual Startup Time': 67.08, 'Actual Total Time': 341.561, 'Actual Rows': 134807, 'Actual Loops': 1, 'Workers Planned': 2, 'Workers Launched': 2, 'Single Copy': False, 'Plans': [{'Node Type': 'Hash Join', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Join Type': 'Inner', 'Startup Cost': 21727.12, 'Total Cost': 73754.4, 'Plan Rows': 54511, 'Plan Width': 134, 'Actual Startup Time': 53.157, 'Actual Total Time': 304.149, 'Actual Rows': 44936, 'Actual Loops': 3, 'Inner Unique': False, 'Hash Cond': '(t.id = mc.movie_id)', 'Workers': [], 'Plans': [{'Node Type': 'Seq Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Relation Name': 'title', 'Alias': 't', 'Startup Cost': 0.0, 'Total Cost': 46532.77, 'Plan Rows': 1053477, 'Plan Width': 94, 'Actual Startup Time': 0.352, 'Actual Total Time': 108.439, 'Actual Rows': 842771, 'Actual Loops': 3, 'Workers': []}, {'Node Type': 'Hash', 'Parent Relationship': 'Inner', 'Parallel Aware': True, 'Startup Cost': 21045.73, 'Total Cost': 21045.73, 'Plan Rows': 54511, 'Plan Width': 40, 'Actual Startup Time': 52.596, 'Actual Total Time': 52.596, 'Actual Rows': 44936, 'Actual Loops': 3, 'Hash Buckets': 262144, 'Original Hash Buckets': 131072, 'Hash Batches': 1, 'Original Hash Batches': 1, 'Peak Memory Usage': 12448, 'Workers': [], 'Plans': [{'Node Type': 'Bitmap Heap Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Relation Name': 'movie_companies', 'Alias': 'mc', 'Startup Cost': 1574.34, 'Total Cost': 21045.73, 'Plan Rows': 54511, 'Plan Width': 40, 'Actual Startup Time': 2.121, 'Actual Total Time': 42.538, 'Actual Rows': 44936, 'Actual Loops': 3, 'Recheck Cond': '(company_id < 27)', 'Rows Removed by Index Recheck': 0, 'Exact Heap Blocks': 4413, 'Lossy Heap Blocks': 0, 'Workers': [], 'Plans': [{'Node Type': 'Bitmap Index Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': False, 'Index Name': 'company_id_movie_companies', 'Startup Cost': 0.0, 'Total Cost': 1541.63, 'Plan Rows': 130827, 'Plan Width': 0, 'Actual Startup Time': 5.26, 'Actual Total Time': 5.26, 'Actual Rows': 134807, 'Actual Loops': 1, 'Index Cond': '(company_id < 27)', 'Workers': []}]}]}]}]}, {'Node Type': 'Bitmap Heap Scan', 'Parallel Aware': False, 'Relation Name': 'movie_keyword', 'Alias': 'mk', 'Startup Cost': 725.65, 'Total Cost': 25968.71, 'Plan Rows': 63125, 'Plan Width': 12, 'Actual Startup Time': 4.896, 'Actual Total Time': 89.198, 'Actual Rows': 54826, 'Actual Loops': 1, 'Recheck Cond': '(keyword_id < 55)', 'Rows Removed by Index Recheck': 0, 'Exact Heap Blocks': 16134, 'Lossy Heap Blocks': 0, 'Plans': [{'Node Type': 'Bitmap Index Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': False, 'Index Name': 'keyword_id_movie_keyword', 'Startup Cost': 0.0, 'Total Cost': 709.87, 'Plan Rows': 63125, 'Plan Width': 0, 'Actual Startup Time': 3.307, 'Actual Total Time': 3.307, 'Actual Rows': 54826, 'Actual Loops': 1, 'Index Cond': '(keyword_id < 55)'}]}, {'Node Type': 'Gather', 'Parallel Aware': False, 'Startup Cost': 120644.8, 'Total Cost': 475278.28, 'Plan Rows': 656491, 'Plan Width': 208, 'Actual Startup Time': 1435.418, 'Actual Total Time': 1939.636, 'Actual Rows': 1172762, 'Actual Loops': 1, 'Workers Planned': 2, 'Workers Launched': 2, 'Single Copy': False, 'Plans': [{'Node Type': 'Hash Join', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Join Type': 'Inner', 'Startup Cost': 119644.8, 'Total Cost': 408629.18, 'Plan Rows': 273538, 'Plan Width': 208, 'Actual Startup Time': 1420.552, 'Actual Total Time': 1681.032, 'Actual Rows': 390921, 'Actual Loops': 3, 'Inner Unique': False, 'Hash Cond': '(mi.movie_id = t.id)', 'Workers': [], 'Plans': [{'Node Type': 'Seq Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Relation Name': 'movie_info', 'Alias': 'mi', 'Startup Cost': 0.0, 'Total Cost': 239266.15, 'Plan Rows': 1541732, 'Plan Width': 74, 'Actual Startup Time': 61.672, 'Actual Total Time': 661.905, 'Actual Rows': 1208326, 'Actual Loops': 3, 'Filter': '(info_type_id < 4)', 'Rows Removed by Filter': 3736914, 'Workers': []}, {'Node Type': 'Hash', 'Parent Relationship': 'Inner', 'Parallel Aware': True, 'Startup Cost': 113657.42, 'Total Cost': 113657.42, 'Plan Rows': 186910, 'Plan Width': 134, 'Actual Startup Time': 597.814, 'Actual Total Time': 597.817, 'Actual Rows': 137521, 'Actual Loops': 3, 'Hash Buckets': 32768, 'Original Hash Buckets': 32768, 'Hash Batches': 32, 'Original Hash Batches': 32, 'Peak Memory Usage': 2048, 'Workers': [], 'Plans': [{'Node Type': 'Hash Join', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Join Type': 'Inner', 'Startup Cost': 33460.02, 'Total Cost': 113657.42, 'Plan Rows': 186910, 'Plan Width': 134, 'Actual Startup Time': 373.849, 'Actual Total Time': 545.051, 'Actual Rows': 137521, 'Actual Loops': 3, 'Inner Unique': False, 'Hash Cond': '(t.id = mc.movie_id)', 'Workers': [], 'Plans': [{'Node Type': 'Seq Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Relation Name': 'title', 'Alias': 't', 'Startup Cost': 0.0, 'Total Cost': 49166.46, 'Plan Rows': 827830, 'Plan Width': 94, 'Actual Startup Time': 0.301, 'Actual Total Time': 148.379, 'Actual Rows': 658872, 'Actual Loops': 3, 'Filter': '(production_year > 1977)', 'Rows Removed by Filter': 183898, 'Workers': []}, {'Node Type': 'Hash', 'Parent Relationship': 'Inner', 'Parallel Aware': True, 'Startup Cost': 28627.8, 'Total Cost': 28627.8, 'Plan Rows': 237858, 'Plan Width': 40, 'Actual Startup Time': 74.939, 'Actual Total Time': 74.94, 'Actual Rows': 189629, 'Actual Loops': 3, 'Hash Buckets': 65536, 'Original Hash Buckets': 65536, 'Hash Batches': 16, 'Original Hash Batches': 16, 'Peak Memory Usage': 2368, 'Workers': [], 'Plans': [{'Node Type': 'Bitmap Heap Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Relation Name': 'movie_companies', 'Alias': 'mc', 'Startup Cost': 6864.58, 'Total Cost': 28627.8, 'Plan Rows': 237858, 'Plan Width': 40, 'Actual Startup Time': 6.688, 'Actual Total Time': 41.118, 'Actual Rows': 189629, 'Actual Loops': 3, 'Recheck Cond': '(company_id > 71403)', 'Rows Removed by Index Recheck': 0, 'Exact Heap Blocks': 2834, 'Lossy Heap Blocks': 0, 'Workers': [], 'Plans': [{'Node Type': 'Bitmap Index Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': False, 'Index Name': 'company_id_movie_companies', 'Startup Cost': 0.0, 'Total Cost': 6721.86, 'Plan Rows': 570858, 'Plan Width': 0, 'Actual Startup Time': 19.298, 'Actual Total Time': 19.298, 'Actual Rows': 568886, 'Actual Loops': 1, 'Index Cond': '(company_id > 71403)', 'Workers': []}]}]}]}]}]}]}, {'Node Type': 'Seq Scan', 'Parallel Aware': False, 'Relation Name': 'movie_keyword', 'Alias': 'mk', 'Startup Cost': 0.0, 'Total Cost': 81003.13, 'Plan Rows': 4010820, 'Plan Width': 12, 'Actual Startup Time': 0.03, 'Actual Total Time': 358.848, 'Actual Rows': 4007119, 'Actual Loops': 1, 'Filter': '(keyword_id < 35049)', 'Rows Removed by Filter': 516811}, {'Node Type': 'Hash Join', 'Parallel Aware': False, 'Join Type': 'Inner', 'Startup Cost': 129922.74, 'Total Cost': 259458.0, 'Plan Rows': 2609129, 'Plan Width': 134, 'Actual Startup Time': 792.7, 'Actual Total Time': 2810.915, 'Actual Rows': 2609129, 'Actual Loops': 1, 'Inner Unique': True, 'Hash Cond': '(mc.movie_id = t.id)', 'Plans': [{'Node Type': 'Seq Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': False, 'Relation Name': 'movie_companies', 'Alias': 'mc', 'Startup Cost': 0.0, 'Total Cost': 44881.29, 'Plan Rows': 2609129, 'Plan Width': 40, 'Actual Startup Time': 0.026, 'Actual Total Time': 157.518, 'Actual Rows': 2609129, 'Actual Loops': 1}, {'Node Type': 'Hash', 'Parent Relationship': 'Inner', 'Parallel Aware': False, 'Startup Cost': 61281.44, 'Total Cost': 61281.44, 'Plan Rows': 2528344, 'Plan Width': 94, 'Actual Startup Time': 787.281, 'Actual Total Time': 787.281, 'Actual Rows': 2528312, 'Actual Loops': 1, 'Hash Buckets': 32768, 'Original Hash Buckets': 32768, 'Hash Batches': 128, 'Original Hash Batches': 128, 'Peak Memory Usage': 2453, 'Plans': [{'Node Type': 'Seq Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': False, 'Relation Name': 'title', 'Alias': 't', 'Startup Cost': 0.0, 'Total Cost': 61281.44, 'Plan Rows': 2528344, 'Plan Width': 94, 'Actual Startup Time': 0.011, 'Actual Total Time': 245.619, 'Actual Rows': 2528312, 'Actual Loops': 1}]}]}, {'Node Type': 'Seq Scan', 'Parallel Aware': False, 'Relation Name': 'movie_info_idx', 'Alias': 'mi_idx', 'Startup Cost': 0.0, 'Total Cost': 25185.44, 'Plan Rows': 920621, 'Plan Width': 25, 'Actual Startup Time': 0.026, 'Actual Total Time': 117.895, 'Actual Rows': 920110, 'Actual Loops': 1, 'Filter': '(info_type_id > 99)', 'Rows Removed by Filter': 459925}, {'Node Type': 'Gather', 'Parallel Aware': False, 'Startup Cost': 109046.17, 'Total Cost': 418961.41, 'Plan Rows': 399294, 'Plan Width': 193, 'Actual Startup Time': 1526.941, 'Actual Total Time': 2174.669, 'Actual Rows': 1603946, 'Actual Loops': 1, 'Workers Planned': 2, 'Workers Launched': 2, 'Single Copy': False, 'Plans': [{'Node Type': 'Hash Join', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Join Type': 'Inner', 'Startup Cost': 108046.17, 'Total Cost': 378032.01, 'Plan Rows': 166372, 'Plan Width': 193, 'Actual Startup Time': 1513.282, 'Actual Total Time': 1818.77, 'Actual Rows': 534649, 'Actual Loops': 3, 'Inner Unique': False, 'Hash Cond': '(mi.movie_id = t.id)', 'Workers': [], 'Plans': [{'Node Type': 'Seq Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Relation Name': 'movie_info', 'Alias': 'mi', 'Startup Cost': 0.0, 'Total Cost': 239266.15, 'Plan Rows': 914793, 'Plan Width': 74, 'Actual Startup Time': 35.775, 'Actual Total Time': 667.613, 'Actual Rows': 744617, 'Actual Loops': 3, 'Filter': '(info_type_id > 16)', 'Rows Removed by Filter': 4200623, 'Workers': []}, {'Node Type': 'Hash', 'Parent Relationship': 'Inner', 'Parallel Aware': True, 'Startup Cost': 102283.24, 'Total Cost': 102283.24, 'Plan Rows': 191595, 'Plan Width': 119, 'Actual Startup Time': 629.442, 'Actual Total Time': 629.444, 'Actual Rows': 153308, 'Actual Loops': 3, 'Hash Buckets': 32768, 'Original Hash Buckets': 32768, 'Hash Batches': 32, 'Original Hash Batches': 32, 'Peak Memory Usage': 2208, 'Workers': [], 'Plans': [{'Node Type': 'Hash Join', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Join Type': 'Inner', 'Startup Cost': 18827.62, 'Total Cost': 102283.24, 'Plan Rows': 191595, 'Plan Width': 119, 'Actual Startup Time': 361.017, 'Actual Total Time': 570.244, 'Actual Rows': 153308, 'Actual Loops': 3, 'Inner Unique': False, 'Hash Cond': '(t.id = mi_idx.movie_id)', 'Workers': [], 'Plans': [{'Node Type': 'Seq Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Relation Name': 'title', 'Alias': 't', 'Startup Cost': 0.0, 'Total Cost': 46532.77, 'Plan Rows': 1053477, 'Plan Width': 94, 'Actual Startup Time': 0.261, 'Actual Total Time': 111.265, 'Actual Rows': 842771, 'Actual Loops': 3, 'Workers': []}, {'Node Type': 'Hash', 'Parent Relationship': 'Inner', 'Parallel Aware': True, 'Startup Cost': 15122.68, 'Total Cost': 15122.68, 'Plan Rows': 191595, 'Plan Width': 25, 'Actual Startup Time': 73.632, 'Actual Total Time': 73.633, 'Actual Rows': 153308, 'Actual Loops': 3, 'Hash Buckets': 65536, 'Original Hash Buckets': 65536, 'Hash Batches': 16, 'Original Hash Batches': 16, 'Peak Memory Usage': 1920, 'Workers': [], 'Plans': [{'Node Type': 'Seq Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Relation Name': 'movie_info_idx', 'Alias': 'mi_idx', 'Startup Cost': 0.0, 'Total Cost': 15122.68, 'Plan Rows': 191595, 'Plan Width': 25, 'Actual Startup Time': 0.171, 'Actual Total Time': 48.758, 'Actual Rows': 153308, 'Actual Loops': 3, 'Filter': '(info_type_id = 100)', 'Rows Removed by Filter': 306703, 'Workers': []}]}]}]}]}]}]\n"
     ]
    }
   ],
   "source": [
    "temp_length = len(temp_df)\n",
    "#nodes = [json.loads(plan) for plan in temp_df['json']]\n",
    "nodes = [json.loads(plan)['Plan'] for plan in temp_df['json']]\n",
    "print(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "25ae6952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c988467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d99472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cards = [node['Actual Rows'] for node in nodes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0097a0d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[283812,\n",
       " 1107925,\n",
       " 3624977,\n",
       " 134807,\n",
       " 54826,\n",
       " 1172762,\n",
       " 4007119,\n",
       " 2609129,\n",
       " 920110,\n",
       " 1603946]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65793b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = [json.loads(plan)['Execution Time'] for plan in temp_df['json']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc06abdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[654.241,\n",
       " 349.797,\n",
       " 1699.24,\n",
       " 345.056,\n",
       " 90.666,\n",
       " 1969.649,\n",
       " 451.819,\n",
       " 2872.724,\n",
       " 139.463,\n",
       " 2214.235]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "98d193b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/db2inst1/sq-wlm/QueryFormer/model/dataset.py:109: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  'features' : torch.FloatTensor(features),\n"
     ]
    }
   ],
   "source": [
    "train_ds = PlanTreeDataset(full_train_df, None, encoding, hist_file, card_norm, cost_norm, to_predict, table_sample)\n",
    "val_ds = PlanTreeDataset(val_df, None, encoding, hist_file, card_norm, cost_norm, to_predict, table_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bdcefceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Avg Loss: 9.878756431862712e-06, Time: 30.722636699676514\n",
      "Median: 2.884076492072395\n",
      "Mean: 7.973215881877905\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'best_model_path' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m crit \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[0;32m----> 2\u001b[0m model, best_path \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcost_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sq-wlm/QueryFormer/model/trainer.py:151\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_ds, val_ds, crit, cost_norm, args, optimizer, scheduler)\u001b[0m\n\u001b[1;32m    147\u001b[0m         train_scores \u001b[38;5;241m=\u001b[39m print_qerror(cost_norm\u001b[38;5;241m.\u001b[39munnormalize_labels(cost_predss),cost_labelss, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    149\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()   \n\u001b[0;32m--> 151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m, best_model_path\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'best_model_path' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "crit = nn.MSELoss()\n",
    "model, best_path = train(model, train_ds, val_ds, crit, cost_norm, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617b4c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [json.loads(plan)['Plan'] for plan in train_ds['json']]\n",
    "idxs = list(train_ds['id'])\n",
    "train_ds = train_ds.traversePlan(idxs[0], nodes[0], train_ds.encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dc344e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f1095a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfa517f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = {\n",
    "    'get_sample' : get_job_table_sample,\n",
    "    'encoding': encoding,\n",
    "    'cost_norm': cost_norm,\n",
    "    'hist_file': hist_file,\n",
    "    'model': model,\n",
    "    'device': args.device,\n",
    "    'bs': 512,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2b14a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9e7796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd89df96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded queries with len  70\n",
      "Loaded bitmaps\n",
      "Median: 1.6015447359157347\n",
      "Mean: 15.04861380976482\n",
      "Corr:  0.8955015382416885\n"
     ]
    }
   ],
   "source": [
    "_ = eval_workload('job-light', methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e40c30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded queries with len  5000\n",
      "Loaded bitmaps\n",
      "Median: 1.0554397104507522\n",
      "Mean: 1.7017223965744472\n",
      "Corr:  0.9835725288032631\n"
     ]
    }
   ],
   "source": [
    "_ = eval_workload('synthetic', methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30aceed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e47dfb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0622ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b92aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ceb39d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "43655846fa5fcb576e7b347c67e5b55502d05625d311516061de87bdb107a802"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
